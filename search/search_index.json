{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to toomanycells","text":"<p>A python package for spectral clustering</p> <ul> <li>Free software: BSD License</li> <li>Documentation: https://JRR3.github.io/toomanycells</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>TODO</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v001-date","title":"v0.0.1 - Date","text":"<p>Improvement:</p> <ul> <li>TBD</li> </ul> <p>New Features:</p> <ul> <li>TBD</li> </ul>"},{"location":"common/","title":"common module","text":""},{"location":"common/#toomanycells.common.MultiIndexList","title":"<code> MultiIndexList            (list)         </code>","text":"<p>This class is derived from the list class.        It allows the use of iterables to         access the list. For example:         L[(1,2,0)] will access item #1         of the list, then item #2 of the         previously retrieved item, and         finally item #0 of that last item.        We use this class to store the         TooManyCells tree in a structure         composed of nested lists and dictionaries.</p> Source code in <code>toomanycells/common.py</code> <pre><code>class MultiIndexList(list):\n    \"\"\"\n    This class is derived from the list class.\\\n        It allows the use of iterables to \\\n        access the list. For example: \\\n        L[(1,2,0)] will access item #1 \\\n        of the list, then item #2 of the \\\n        previously retrieved item, and \\\n        finally item #0 of that last item.\\\n        We use this class to store the \\\n        TooManyCells tree in a structure \\\n        composed of nested lists and dictionaries.\n    \"\"\"\n    #=================================================\n    def __getitem__(self, indices: Union[list,int]):\n        \"\"\"\n        This implementation of the __getitem__ method \\\n            allows the possibility of indexing a nested \\\n            list with a list of integers.\n        \"\"\"\n\n        if hasattr(indices, '__iter__'):\n            #If the indices object is iterable\n            #then traverse the list using the indices.\n            obj = self\n            for index in indices:\n                obj = obj[index]\n            return obj\n        else:\n            #Otherwise, just use the __getitem__ \n            #method of the parent class.\n            return super().__getitem__(indices)\n</code></pre>"},{"location":"common/#toomanycells.common.MultiIndexList.__getitem__","title":"<code>__getitem__(self, indices)</code>  <code>special</code>","text":"<p>This implementation of the getitem method             allows the possibility of indexing a nested             list with a list of integers.</p> Source code in <code>toomanycells/common.py</code> <pre><code>def __getitem__(self, indices: Union[list,int]):\n    \"\"\"\n    This implementation of the __getitem__ method \\\n        allows the possibility of indexing a nested \\\n        list with a list of integers.\n    \"\"\"\n\n    if hasattr(indices, '__iter__'):\n        #If the indices object is iterable\n        #then traverse the list using the indices.\n        obj = self\n        for index in indices:\n            obj = obj[index]\n        return obj\n    else:\n        #Otherwise, just use the __getitem__ \n        #method of the parent class.\n        return super().__getitem__(indices)\n</code></pre>"},{"location":"common/#toomanycells.common.load_metadata_for_demo","title":"<code>load_metadata_for_demo()</code>","text":"<p>This function loads the cell indices and     labels for the demo file.</p> Source code in <code>toomanycells/common.py</code> <pre><code>def load_metadata_for_demo()-&gt; pd.DataFrame:\n    \"\"\"\n    This function loads the cell indices and \\\n    labels for the demo file.\n    \"\"\"\n    fname = os.path.dirname(__file__)\n    fname = os.path.join(fname, \"data\")\n    fname = os.path.join(fname, \"metadata.csv\")\n    df = pd.read_csv(fname, index_col = 0)\n    return df\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/JRR3/toomanycells/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>toomanycells could always use more documentation, whether as part of the official toomanycells docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/JRR3/toomanycells/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up toomanycells for local development.</p> <ol> <li> <p>Fork the toomanycells repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/toomanycells.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv toomanycells\n$ cd toomanycells/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 toomanycells tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.rst.</li> <li>The pull request should work for Python 3.8 and later, and     for PyPy. Check https://github.com/JRR3/toomanycells/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"faq/","title":"FAQ","text":""},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install toomanycells, run this command in your terminal:</p> <pre><code>pip install toomanycells\n</code></pre> <p>This is the preferred method to install toomanycells, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-sources","title":"From sources","text":"<p>To install toomanycells from sources, run this command in your terminal:</p> <pre><code>pip install git+https://github.com/JRR3/toomanycells\n</code></pre>"},{"location":"toomanycells/","title":"toomanycells module","text":""},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells","title":"<code> TooManyCells        </code>","text":"<p>This class focuses on one aspect of the original         Too-Many-Cells tool, the clustering.\\      Features such as normalization,         dimensionality reduction and many others can be         applied using functions from libraries like \\      Scanpy, or they can be implemented locally. This         implementation also allows the possibility of         new features with respect to the original         Too-Many-Cells. For example, imagine you want to         continue partitioning fibroblasts until you have         at most a given number of cells, even if the         modularity becomes negative, but for CD8+ T-cells         you do not want to have partitions with less         than 100 cells. This can be easily implemented         with a few conditions using the cell annotations         in the .obs data frame of the AnnData object. With regards to visualization, we recommend         using the too-many-cells-interactive tool.         You can find it at:\\      https://github.com/schwartzlab-methods/        too-many-cells-interactive.git        Once installed, you can use the function         visualize_with_tmc_interactive() to         generate the visualization. You will need         path to the installation folder of         too-many-cells-interactive.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>class TooManyCells:\n    \"\"\"\n    This class focuses on one aspect of the original \\\n        Too-Many-Cells tool, the clustering.\\ \n        Features such as normalization, \\\n        dimensionality reduction and many others can be \\\n        applied using functions from libraries like \\ \n        Scanpy, or they can be implemented locally. This \\\n        implementation also allows the possibility of \\\n        new features with respect to the original \\\n        Too-Many-Cells. For example, imagine you want to \\\n        continue partitioning fibroblasts until you have \\\n        at most a given number of cells, even if the \\\n        modularity becomes negative, but for CD8+ T-cells \\\n        you do not want to have partitions with less \\\n        than 100 cells. This can be easily implemented \\\n        with a few conditions using the cell annotations \\\n        in the .obs data frame of the AnnData object.\\\n\n    With regards to visualization, we recommend \\\n        using the too-many-cells-interactive tool. \\\n        You can find it at:\\ \n        https://github.com/schwartzlab-methods/\\\n        too-many-cells-interactive.git\\\n        Once installed, you can use the function \\\n        visualize_with_tmc_interactive() to \\\n        generate the visualization. You will need \\\n        path to the installation folder of \\\n        too-many-cells-interactive.\n\n\n    \"\"\"\n    #=================================================\n    def __init__(self,\n            input: Union[AnnData, str],\n            output: Optional[str] = \"\",\n            input_is_matrix_market: Optional[bool] = False,\n            use_full_matrix: Optional[bool] = False,\n            ):\n        \"\"\"\n        The constructor takes the following inputs.\n\n        :param input: Path to input directory or \\\n                AnnData object.\n        :param output: Path to output directory.\n        :param input_is_matrix_market: If true, \\\n                the directory should contain a \\\n                .mtx file, a barcodes.tsv file \\\n                and a genes.tsv file.\n\n        :return: a TooManyCells object.\n        :rtype: :obj:`TooManyCells`\n\n        \"\"\"\n\n        #We use a directed graph to enforce the parent\n        #to child relation.\n        self.G = nx.DiGraph()\n\n        self.set_of_leaf_nodes = set()\n\n        if isinstance(input, TooManyCells):\n\n            #Clone the given TooManyCells object.\n            self.A = input.A.copy()\n            self.G = input.G.copy()\n            S = input.set_of_leaf_nodes.copy()\n            self.set_of_leaf_nodes = S\n\n        elif isinstance(input, str):\n            self.source = os.path.abspath(input)\n            if self.source.endswith('.h5ad'):\n                self.t0 = clock()\n                self.A = ad.read_h5ad(self.source)\n                self.tf = clock()\n                delta = self.tf - self.t0\n                txt = ('Elapsed time for loading: ' +\n                        f'{delta:.2f} seconds.')\n                print(txt)\n            else:\n                if input_is_matrix_market:\n                    self.convert_mm_from_source_to_anndata()\n                else:\n                    for f in os.listdir(self.source):\n                        if f.endswith('.h5ad'):\n                            fname = os.path.join(\n                                self.source, f)\n                            self.t0 = clock()\n                            self.A = ad.read_h5ad(fname)\n                            self.tf = clock()\n                            delta = self.tf - self.t0\n                            txt = ('Elapsed time for ' +\n                                   'loading: ' +\n                                    f'{delta:.2f} seconds.')\n                            print(txt)\n                            break\n\n        elif isinstance(input, AnnData):\n            self.A = input\n        else:\n            raise ValueError('Unexpected input type.')\n\n        #If no output directory is provided,\n        #we use the current working directory.\n        if output == \"\":\n            output = os.getcwd()\n            output = os.path.join(output, \"tmc_outputs\")\n            print(f\"Outputs will be saved in: {output}\")\n\n        if not os.path.exists(output):\n            os.makedirs(output)\n\n        self.output = os.path.abspath(output)\n\n        #This column of the obs data frame indicates\n        #the correspondence between a cell and the \n        #leaf node of the spectral clustering tree.\n        sp_cluster = \"sp_cluster\"\n        sp_path = \"sp_path\"\n        if sp_cluster not in self.A.obs.columns:\n            self.A.obs[sp_cluster] = -1\n        if sp_path not in self.A.obs.columns:\n            self.A.obs[sp_path]    = \"\"\n\n        t = self.A.obs.columns.get_loc(sp_cluster)\n        self.cluster_column_index = t\n        t = self.A.obs.columns.get_loc(sp_path)\n        self.path_column_index = t\n\n        self.delta_clustering = 0\n        self.final_n_iter     = 0\n\n        #Create a copy to avoid direct modifications\n        #of the original count matrix X.\n        #Note that we are making sure that the \n        #sparse matrix has the CSR format. This\n        #is relevant when we normalize.\n        if sp.issparse(self.A.X):\n            #Compute the density of the matrix\n            rho = self.A.X.nnz / np.prod(self.A.X.shape)\n            #If more than 50% of the matrix is occupied,\n            #we generate a dense version of the matrix.\n            sparse_threshold = 0.50\n            if use_full_matrix or sparse_threshold &lt; rho:\n                self.is_sparse = False\n                self.X = self.A.X.toarray()\n                txt = (\"Using a dense representation\" \n                       \" of the count matrix.\")\n                print(txt)\n                txt = (\"Values will be converted to\" \n                       \" float32.\")\n                print(txt)\n                self.X = self.X.astype(np.float32)\n            else:\n                self.is_sparse = True\n                #Make sure we use a CSR format.\n                self.X = sp.csr_matrix(self.A.X,\n                                       dtype=np.float32,\n                                       copy=True)\n        else:\n            #The matrix is dense.\n            print(\"The matrix is dense.\")\n            self.is_sparse = False\n            self.X = self.A.X.copy()\n            txt = (\"Values will be converted to\" \n                   \" float32.\")\n            print(txt)\n            self.X = self.X.astype(np.float32)\n\n        self.n_cells, self.n_genes = self.A.shape\n\n        if self.n_cells &lt; 3:\n            raise ValueError(\"Too few observations (cells).\")\n\n        print(self.A)\n\n        #Location of the matrix data for TMCI\n        self.tmci_mtx_dir = \"\"\n\n        self.spectral_clustering_has_been_called = False\n\n        self.cells_to_be_eliminated = None\n\n        x = \"/home/javier/Documents/repos/too-many-cells-interactive\"\n\n        # We use a deque to offer the possibility of breadth-\n        # versus depth-first. Our current implementation\n        # uses depth-first to be consistent with the \n        # numbering scheme of TooManyCellsInteractive.\n        self.DQ = deque()\n\n\n        #Map a node to the path in the\n        #binary tree that connects the\n        #root node to the given node.\n        self.node_to_path = {}\n\n        #Map a node to a list of indices\n        #that provide access to the JSON\n        #structure.\n        self.node_to_j_index = {}\n\n        #the JSON structure representation\n        #of the tree.\n        self.J = MultiIndexList()\n\n        self.node_counter = 0\n\n        #The threshold for modularity to \n        #accept a given partition of a set\n        #of cells.\n        self.eps = 1e-9\n\n        self.use_twopi_cmd   = True\n        self.verbose_mode    = False\n\n    #=====================================\n    def normalize_sparse_rows(self):\n        \"\"\"\n        Divide each row of the count matrix by the \\\n            given norm. Note that this function \\\n            assumes that the matrix is in the \\\n            compressed sparse row format.\n        \"\"\"\n\n        print(\"Normalizing rows.\")\n\n\n        #It's just an alias.\n        mat = self.X\n\n        for i in range(self.n_cells):\n            row = mat.getrow(i)\n            nz = row.data\n            row_norm  = np.linalg.norm(\n                nz, ord=self.similarity_norm)\n            row = nz / row_norm\n            mat.data[mat.indptr[i]:mat.indptr[i+1]] = row\n\n    #=====================================\n    def normalize_dense_rows(self):\n        \"\"\"\n        Divide each row of the count matrix by the \\\n            given norm. Note that this function \\\n            assumes that the matrix is dense.\n        \"\"\"\n\n        print('Normalizing rows.')\n\n        for row in self.X:\n            row /= np.linalg.norm(row,\n                                  ord=self.similarity_norm)\n\n    #=====================================\n    def modularity_to_json(self,Q):\n        return {'_item': None,\n                '_significance': None,\n                '_distance': Q}\n\n    #=====================================\n    def cell_to_json(self, cell_name, cell_number):\n        return {'_barcode': {'unCell': cell_name},\n                '_cellRow': {'unRow': cell_number}}\n\n    #=====================================\n    def cells_to_json(self,rows):\n        L = []\n        for row in rows:\n            cell_id = self.A.obs.index[row]\n            D = self.cell_to_json(cell_id, row)\n            L.append(D)\n        return {'_item': L,\n                '_significance': None,\n                '_distance': None}\n\n    #=====================================\n    def estimate_n_of_iterations(self) -&gt; int:\n        \"\"\"\n        We assume a model of the form \\\n        number_of_iter = const * N^exponent \\\n        where N is the number of cells.\n        \"\"\"\n\n        #Average number of cells per leaf node\n        k = np.power(10, -0.6681664297844971)\n        exponent = 0.86121348\n        #exponent = 0.9\n        q1 = k * np.power(self.n_cells, exponent)\n        q2 = 2\n        iter_estimates = np.array([q1,q2], dtype=int)\n\n        return iter_estimates.max()\n\n    #=====================================\n    def print_message_before_clustering(self):\n\n        print(\"The first iterations are typically slow.\")\n        print(\"However, they tend to become faster as \")\n        print(\"the size of the partition becomes smaller.\")\n        print(\"Note that the number of iterations is\")\n        print(\"only an estimate.\")\n    #=====================================\n    def reverse_path(self, p: str)-&gt;str:\n        \"\"\"\n        This function reverses the path from the root\\\n        node to the leaf node.\n        \"\"\"\n        reversed_p = \"/\".join(p.split(\"/\")[::-1])\n        return reversed_p\n\n    #=====================================\n    def run_spectral_clustering(\n            self,\n            shift_similarity_matrix:Optional[float] = 0,\n            normalize_rows:Optional[bool] = False,\n            similarity_function:Optional[str]=\"cosine_sparse\",\n            similarity_norm: Optional[float] = 2,\n            similarity_power: Optional[float] = 1,\n            similarity_gamma: Optional[float] = None,\n            use_eig_decomp: Optional[bool] = False,\n            use_tf_idf: Optional[bool] = False,\n            tf_idf_norm: Optional[str] = None,\n            tf_idf_smooth: Optional[str] = True,\n            svd_algorithm: Optional[str] = \"randomized\"):\n        \"\"\"\n        This function computes the partitions of the \\\n                initial cell population and continues \\\n                until the modularity of the newly \\\n                created partitions is nonpositive.\n        \"\"\"\n\n        svd_algorithms = [\"randomized\",\"arpack\"]\n        if svd_algorithm not in svd_algorithms:\n            raise ValueError(\"Unexpected SVD algorithm.\")\n\n        if similarity_norm &lt; 1:\n            raise ValueError(\"Unexpected similarity norm.\")\n        self.similarity_norm = similarity_norm\n\n        if similarity_gamma is None:\n            # gamma = 1 / (number of features)\n            similarity_gamma = 1 / self.X.shape[1]\n        elif similarity_gamma &lt;= 0:\n            raise ValueError(\"Unexpected similarity gamma.\")\n\n        if similarity_power &lt;= 0:\n            raise ValueError(\"Unexpected similarity power.\")\n\n        similarity_functions = []\n        similarity_functions.append(\"cosine_sparse\")\n        similarity_functions.append(\"cosine\")\n        similarity_functions.append(\"neg_exp\")\n        similarity_functions.append(\"laplacian\")\n        similarity_functions.append(\"gaussian\")\n        similarity_functions.append(\"div_by_sum\")\n        if similarity_function not in similarity_functions:\n            raise ValueError(\"Unexpected similarity fun.\")\n\n\n        #In case the user wants to call this function again.\n        self.spectral_clustering_has_been_called = True\n\n        #TF-IDF section\n        if use_tf_idf:\n\n            t0 = clock()\n            print(\"Using inverse document frequency (IDF).\")\n\n            if tf_idf_norm is None:\n                pass \n            else:\n                print(\"Using term frequency normalization.\")\n                tf_idf_norms = [\"l2\",\"l1\"]\n                if tf_idf_norm not in tf_idf_norms:\n                    raise ValueError(\"Unexpected tf norm.\")\n\n            tf_idf_obj = TfidfTransformer(\n                norm=tf_idf_norm,\n                smooth_idf=tf_idf_smooth)\n\n            self.X = tf_idf_obj.fit_transform(self.X)\n            if self.is_sparse:\n                pass\n            else:\n                #If the matrix was originally dense\n                #and the tf_idf function changed it\n                #to sparse, then convert to dense.\n                if sp.issparse(self.X):\n                    self.X = self.X.toarray()\n\n            tf = clock()\n            delta = tf - t0\n            txt = (\"Elapsed time for IDF build: \" +\n                    f\"{delta:.2f} seconds.\")\n            print(txt)\n\n        #Normalization section\n        use_cos_sp = similarity_function == \"cosine_sparse\"\n        use_dbs = similarity_function == \"div_by_sum\"\n        if normalize_rows or use_cos_sp or use_dbs:\n            t0 = clock()\n\n            if self.is_sparse:\n                self.normalize_sparse_rows()\n            else:\n                self.normalize_dense_rows()\n\n            tf = clock()\n            delta = tf - t0\n            txt = (\"Elapsed time for normalization: \" +\n                    f\"{delta:.2f} seconds.\")\n            print(txt)\n\n        #Similarity section.\n        print(f\"Working with {similarity_function=}\")\n\n        if similarity_function == \"cosine_sparse\":\n\n            self.trunc_SVD = TruncatedSVD(\n                    n_components=2,\n                    n_iter=5,\n                    algorithm=svd_algorithm)\n\n        else:\n            #Use a similarity function different from\n            #the cosine_sparse similarity function.\n\n            t0 = clock()\n            print(\"Building similarity matrix ...\")\n            n_rows = self.X.shape[0]\n            max_workers = os.cpu_count()\n            n_workers = 1\n            if n_rows &lt; 500:\n                pass\n            elif n_rows &lt; 5000:\n                if 8 &lt; max_workers:\n                    n_workers = 8\n            elif n_rows &lt; 50000:\n                if 16 &lt; max_workers:\n                    n_workers = 16\n            else:\n                if 25 &lt; max_workers:\n                    n_workers = 25\n            print(f\"Using {n_workers=}.\")\n\n        if similarity_function == \"cosine_sparse\":\n            pass\n        elif similarity_function == \"cosine\":\n            #( x @ y ) / ( ||x|| * ||y|| )\n            def sim_fun(x,y):\n                cos_sim = x @ y\n                x_norm = np.linalg.norm(x, ord=2)\n                y_norm = np.linalg.norm(y, ord=2)\n                cos_sim /= (x_norm * y_norm)\n                return cos_sim\n\n            self.X = pairwise_kernels(self.X,\n                                        metric=\"cosine\",\n                                        n_jobs=n_workers)\n\n        elif similarity_function == \"neg_exp\":\n            #exp(-||x-y||^power * gamma)\n            def sim_fun(x,y):\n                delta = np.linalg.norm(\n                    x-y, ord=similarity_norm)\n                delta = np.power(delta, similarity_power)\n                return np.exp(-delta * similarity_gamma)\n\n            self.X = pairwise_kernels(\n                self.X,\n                metric=sim_fun,\n                n_jobs=n_workers)\n\n        elif similarity_function == \"laplacian\":\n            #exp(-||x-y||^power * gamma)\n            def sim_fun(x,y):\n                delta = np.linalg.norm(\n                    x-y, ord=1)\n                delta = np.power(delta, 1)\n                return np.exp(-delta * similarity_gamma)\n\n            self.X = pairwise_kernels(\n                self.X,\n                metric=\"laplacian\",\n                n_jobs=n_workers,\n                gamma = similarity_gamma)\n\n        elif similarity_function == \"gaussian\":\n            #exp(-||x-y||^power * gamma)\n            def sim_fun(x,y):\n                delta = np.linalg.norm(\n                    x-y, ord=2)\n                delta = np.power(delta, 2)\n                return np.exp(-delta * similarity_gamma)\n\n            self.X = pairwise_kernels(\n                self.X,\n                metric=\"rbf\",\n                n_jobs=n_workers,\n                gamma = similarity_gamma)\n\n        elif similarity_function == \"div_by_sum\":\n            #1 - ( ||x-y|| / (||x|| + ||y||) )^power\n            #The rows should have been previously normalized.\n            def sim_fun(x,y):\n                delta = np.linalg.norm(\n                    x-y, ord=similarity_norm)\n                x_norm = np.linalg.norm(\n                    x, ord=similarity_norm)\n                y_norm = np.linalg.norm(\n                    y, ord=similarity_norm)\n                delta /= (x_norm + y_norm)\n                delta = np.power(delta, 1)\n                value =  1 - delta\n                return value\n\n            if self.similarity_norm == 1:\n                lp_norm = \"l1\"\n            elif self.similarity_norm == 2:\n                lp_norm = \"l2\"\n            else:\n                txt = \"Similarity norm should be 1 or 2.\"\n                raise ValueError(txt)\n\n            self.X = pairwise_distances(self.X,\n                                        metric=lp_norm,\n                                        n_jobs=n_workers)\n            self.X *= -0.5\n            self.X += 1\n\n        if similarity_function != \"cosine_sparse\":\n\n            if shift_similarity_matrix != 0:\n                print(f\"Similarity matrix will be shifted.\")\n                print(f\"Shift: {shift_similarity_matrix}.\")\n                self.X += shift_similarity_matrix\n\n            print(\"Similarity matrix has been built.\")\n            tf = clock()\n            delta = tf - t0\n            delta /= 60\n            txt = (\"Elapsed time for similarity build: \" +\n                    f\"{delta:.2f} minutes.\")\n            print(txt)\n\n\n        self.use_eig_decomp = use_eig_decomp\n\n        self.t0 = clock()\n\n        #===========================================\n        #=============Main=Loop=====================\n        #===========================================\n        node_id = self.node_counter\n\n        #Initialize the array of cells to partition\n        rows = np.array(range(self.X.shape[0]))\n\n        #Initialize the deque\n        # self.DQ.append((rows, None))\n        # self.DQ.append(rows)\n\n        #Initialize the graph\n        self.G.add_node(node_id, size=len(rows))\n\n        #Path to reach root node.\n        self.node_to_path[node_id] = str(node_id)\n\n        #Indices to reach root node.\n        self.node_to_j_index[node_id] = (1,)\n\n        #Update the node counter\n        self.node_counter += 1\n\n        #============STEP=1================Cluster(0)\n\n        p_node_id = node_id\n\n        if similarity_function == \"cosine_sparse\":\n            Q,S = self.compute_partition_for_sp(rows)\n        else:\n            Q,S = self.compute_partition_for_gen(rows)\n\n        if self.eps &lt; Q:\n            #Modularity is above threshold, and\n            #thus each partition will be \n            #inserted into the deque.\n\n            D = self.modularity_to_json(Q)\n\n            #Update json index\n            self.J.append(D)\n            self.J.append([])\n            # self.J.append([[],[]])\n            # j_index = (1,)\n\n            self.G.nodes[node_id]['Q'] = Q\n\n            for indices in S:\n                T = (indices, p_node_id)\n                self.DQ.append(T)\n\n        else:\n            #Modularity is below threshold and \n            #therefore this partition will not\n            #be considered.\n            txt = (\"All cells belong\" \n                    \" to the same partition.\")\n            print(txt)\n            return\n\n        max_n_iter = self.estimate_n_of_iterations()\n\n        self.print_message_before_clustering()\n\n        with tqdm(total=max_n_iter) as pbar:\n            while 0 &lt; len(self.DQ):\n\n                #Get the rows corresponding to the\n                #partition and the (parent) node\n                #that produced such partition.\n                rows, p_node_id = self.DQ.pop()\n\n                #This id is for the new node.\n                node_id += 1\n\n                # For every cluster of cells that is popped\n                # from the deque, we update the node_id. \n                # If the cluster is further partitioned we \n                # will store each partition but will not \n                # assign node numbers. Node numbers will \n                # only be assigned after being popped from \n                # the deque.\n\n                # We need to know the modularity to \n                # determine if the node will \n                if similarity_function == \"cosine_sparse\":\n                    Q,S = self.compute_partition_for_sp(rows)\n                else:\n                    Q,S = self.compute_partition_for_gen(rows)\n\n                # If the parent node is 0, then the path is\n                # \"0\".\n                current_path = self.node_to_path[p_node_id]\n\n                #Update path for the new node\n                new_path = current_path \n                new_path += '/' + str(node_id) \n                self.node_to_path[node_id]=new_path\n\n                # If the parent node is 0, then j_index is\n                # (1,)\n                j_index = self.node_to_j_index[p_node_id]\n\n                n_stored_blocks = len(self.J[j_index])\n                self.J[j_index].append([])\n                #Update the j_index. For example, if\n                #j_index = (1,) and no blocks have been\n                #stored, then the new j_index is (1,0).\n                #Otherwise, it is (1,1).\n                j_index += (n_stored_blocks,)\n\n                #Include new node into the graph.\n                self.G.add_node(node_id, size=len(rows))\n\n                #Include new edge into the graph.\n                self.G.add_edge(p_node_id, node_id)\n\n                if self.eps &lt; Q:\n                    #Modularity is above threshold, and\n                    #thus each partition will be \n                    #inserted into the deque.\n\n                    D = self.modularity_to_json(Q)\n                    self.J[j_index].append(D)\n                    self.J[j_index].append([])\n                    j_index += (1,)\n\n                    # We only store the modularity of nodes\n                    # whose modularity is above threshold.\n                    self.G.nodes[node_id]['Q'] = Q\n\n                    # Update the j_index for the newly \n                    # created node. (1,0,1)\n                    self.node_to_j_index[node_id] = j_index\n\n                    # Append each partition to the deque.\n                    for indices in S:\n                        T = (indices, node_id)\n                        self.DQ.append(T)\n\n                else:\n                    #Modularity is below threshold and \n                    #therefore this partition will not\n                    #be considered.\n\n                    #Update the relation between a set of\n                    #cells and the corresponding leaf node.\n                    #Also include the path to reach that node.\n                    c = self.cluster_column_index\n                    self.A.obs.iloc[rows, c] = node_id\n\n                    reversed_path = self.reverse_path(\n                        new_path)\n                    p = self.path_column_index\n                    self.A.obs.iloc[rows, p] = reversed_path\n\n                    self.set_of_leaf_nodes.add(node_id)\n\n                    #Update the JSON structure for \n                    #a leaf node.\n                    L = self.cells_to_json(rows)\n                    self.J[j_index].append(L)\n                    self.J[j_index].append([])\n\n                pbar.update()\n\n            #==============END OF WHILE==============\n            pbar.total = pbar.n\n            self.final_n_iter = pbar.n\n            pbar.refresh()\n\n        self.tf = clock()\n        self.delta_clustering = self.tf - self.t0\n        self.delta_clustering /= 60\n        txt = (\"Elapsed time for clustering: \" +\n                f\"{self.delta_clustering:.2f} minutes.\")\n        print(txt)\n\n    #=====================================\n    def compute_partition_for_sp(self, rows: np.ndarray\n    ) -&gt; tuple:\n    #) -&gt; tuple[float, np.ndarray]:\n        \"\"\"\n        Compute the partition of the given set\\\n            of cells. The rows input \\\n            contains the indices of the \\\n            rows we are to partition. \\\n            The algorithm computes a truncated \\\n            SVD and the corresponding modularity \\\n            of the newly created communities.\n        \"\"\"\n\n        if self.verbose_mode:\n            print(f'I was given: {rows=}')\n\n        partition = []\n        Q = 0\n\n        n_rows = len(rows) \n        #print(f\"Number of cells: {n_rows}\")\n\n        #If the number of rows is less than 3,\n        #we keep the cluster as it is.\n        if n_rows &lt; 3:\n            return (Q, partition)\n\n        B = self.X[rows,:]\n        ones = np.ones(n_rows)\n        partial_row_sums = B.T.dot(ones)\n        #1^T @ B @ B^T @ 1 = (B^T @ 1)^T @ (B^T @ 1)\n        L = partial_row_sums @ partial_row_sums - n_rows\n        #These are the row sums of the similarity matrix\n        row_sums = B @ partial_row_sums\n        #Check if we have negative entries before computing\n        #the square root.\n        # if  neg_row_sums or self.use_eig_decomp:\n        zero_row_sums_mask = np.abs(row_sums) &lt; self.eps\n        has_zero_row_sums = zero_row_sums_mask.any()\n        has_neg_row_sums = (row_sums &lt; -self.eps).any() \n\n        if has_zero_row_sums:\n            print(\"We have zero row sums.\")\n            row_sums[zero_row_sums_mask] = 0\n\n        if has_neg_row_sums and has_zero_row_sums:\n            txt = \"This matrix cannot be processed.\"\n            print(txt)\n            txt = \"Cannot have negative and zero row sums.\"\n            raise ValueError(txt)\n\n        if  has_neg_row_sums:\n            #This means we cannot use the fast approach\n            #We'll have to build a dense representation\n            # of the similarity matrix.\n            if 5000 &lt; n_rows:\n                print(\"The row sums are negative.\")\n                print(\"We will use a full eigen decomp.\")\n                print(f\"The block size is {n_rows}.\")\n                print(\"Warning ...\")\n                txt = \"This operation is very expensive.\"\n                print(txt)\n            laplacian_mtx  = B @ B.T\n            row_sums_mtx   = sp.diags(row_sums)\n            laplacian_mtx  = row_sums_mtx - laplacian_mtx\n\n            #This is a very expensive operation\n            #since it computes all the eigenvectors.\n            inv_row_sums   = 1/row_sums\n            inv_row_sums   = sp.diags(inv_row_sums)\n            laplacian_mtx  = inv_row_sums @ laplacian_mtx\n            eig_obj = np.linalg.eig(laplacian_mtx)\n            eig_vals = eig_obj.eigenvalues\n            eig_vecs = eig_obj.eigenvectors\n            idx = np.argsort(np.abs(np.real(eig_vals)))\n            #Get the index of the second smallest eigenvalue.\n            idx = idx[1]\n            W = np.real(eig_vecs[:,idx])\n            W = np.squeeze(np.asarray(W))\n\n        elif self.use_eig_decomp or has_zero_row_sums:\n            laplacian_mtx  = B @ B.T\n            row_sums_mtx   = sp.diags(row_sums)\n            laplacian_mtx  = row_sums_mtx - laplacian_mtx\n            try:\n                #if the row sums are negative, this \n                #step could fail.\n                E_obj = Eigen_Hermitian(laplacian_mtx,\n                                        k=2,\n                                        M=row_sums_mtx,\n                                        sigma=0,\n                                        which=\"LM\")\n                eigen_val_abs = np.abs(E_obj[0])\n                #Identify the eigenvalue with the\n                #largest magnitude.\n                idx = np.argmax(eigen_val_abs)\n                #Choose the eigenvector corresponding\n                # to the eigenvalue with the \n                # largest magnitude.\n                eigen_vectors = E_obj[1]\n                W = eigen_vectors[:,idx]\n            except:\n                #This is a very expensive operation\n                #since it computes all the eigenvectors.\n                if 5000 &lt; n_rows:\n                    print(\"We will use a full eigen decomp.\")\n                    print(f\"The block size is {n_rows}.\")\n                    print(\"Warning ...\")\n                    txt = \"This operation is very expensive.\"\n                    print(txt)\n                inv_row_sums   = 1/row_sums\n                inv_row_sums   = sp.diags(inv_row_sums)\n                laplacian_mtx  = inv_row_sums @ laplacian_mtx\n                eig_obj = np.linalg.eig(laplacian_mtx)\n                eig_vals = eig_obj.eigenvalues\n                eig_vecs = eig_obj.eigenvectors\n                idx = np.argsort(np.abs(np.real(eig_vals)))\n                idx = idx[1]\n                W = np.real(eig_vecs[:,idx])\n                W = np.squeeze(np.asarray(W))\n\n\n        else:\n            #This is the fast approach.\n            #It is fast in the sense that the \n            #operations are faster if the matrix\n            #is sparse, i.e., O(n) nonzero entries.\n\n            d = 1/np.sqrt(row_sums)\n            D = sp.diags(d)\n            C = D @ B\n            W = self.trunc_SVD.fit_transform(C)\n            singular_values = self.trunc_SVD.singular_values_\n            idx = np.argsort(singular_values)\n            #Get the singular vector corresponding to the\n            #second largest singular value.\n            W = W[:,idx[0]]\n\n\n        mask_c1 = 0 &lt; W\n        mask_c2 = ~mask_c1\n\n        #If one partition has all the elements\n        #then return with Q = 0.\n        if mask_c1.all() or mask_c2.all():\n            return (Q, partition)\n\n        masks = [mask_c1, mask_c2]\n\n        for mask in masks:\n            n_rows_msk = mask.sum()\n            partition.append(rows[mask])\n            ones_msk = ones * mask\n            row_sums_msk = B.T.dot(ones_msk)\n            O_c = row_sums_msk @ row_sums_msk - n_rows_msk\n            L_c = ones_msk @ row_sums  - n_rows_msk\n            Q += O_c / L - (L_c / L)**2\n\n        if self.verbose_mode:\n            print(f'{Q=}')\n            print(f'I found: {partition=}')\n            print('===========================')\n\n        return (Q, partition)\n\n    #=====================================\n    def compute_partition_for_gen(self, rows: np.ndarray\n    ) -&gt; tuple:\n    #) -&gt; tuple[float, np.ndarray]:\n        \"\"\"\n        Compute the partition of the given set\\\n            of cells. The rows input \\\n            contains the indices of the \\\n            rows we are to partition. \\\n            The algorithm computes a truncated \\\n            SVD and the corresponding modularity \\\n            of the newly created communities.\n        \"\"\"\n\n        if self.verbose_mode:\n            print(f'I was given: {rows=}')\n\n        partition = []\n        Q = 0\n\n        n_rows = len(rows) \n        #print(f\"Number of cells: {n_rows}\")\n\n        #If the number of rows is less than 3,\n        #we keep the cluster as it is.\n        if n_rows &lt; 3:\n            return (Q, partition)\n\n        S = self.X[np.ix_(rows, rows)]\n        ones = np.ones(n_rows)\n        row_sums = S.dot(ones)\n        row_sums_mtx   = sp.diags(row_sums)\n        laplacian_mtx  = row_sums_mtx - S\n        L = np.sum(row_sums) - n_rows\n\n        zero_row_sums_mask = np.abs(row_sums) &lt; self.eps\n        has_zero_row_sums = zero_row_sums_mask.any()\n        has_neg_row_sums = (row_sums &lt; -self.eps).any() \n\n        if has_zero_row_sums:\n            print(\"We have zero row sums.\")\n            row_sums[zero_row_sums_mask] = 0\n\n        if has_neg_row_sums and has_zero_row_sums:\n            txt = \"This matrix cannot be processed.\"\n            print(txt)\n            txt = \"Cannot have negative and zero row sums.\"\n            raise ValueError(txt)\n\n        if has_neg_row_sums:\n            #This is a very expensive operation\n            #since it computes all the eigenvectors.\n            if 5000 &lt; n_rows:\n                print(\"The row sums are negative.\")\n                print(\"We will use a full eigen decomp.\")\n                print(f\"The block size is {n_rows}.\")\n                print(\"Warning ...\")\n                txt = \"This operation is very expensive.\"\n                print(txt)\n            inv_row_sums   = 1/row_sums\n            inv_row_sums   = sp.diags(inv_row_sums)\n            laplacian_mtx  = inv_row_sums @ laplacian_mtx\n            eig_obj = np.linalg.eig(laplacian_mtx)\n            eig_vals = eig_obj.eigenvalues\n            eig_vecs = eig_obj.eigenvectors\n            idx = np.argsort(np.abs(np.real(eig_vals)))\n            idx = idx[1]\n            W = np.real(eig_vecs[:,idx])\n            W = np.squeeze(np.asarray(W))\n\n        else:\n            #Nonnegative row sums.\n            try:\n                E_obj = Eigen_Hermitian(laplacian_mtx,\n                                        k=2,\n                                        M=row_sums_mtx,\n                                        sigma=0,\n                                        which=\"LM\")\n                eigen_val_abs = np.abs(E_obj[0])\n                #Identify the eigenvalue with the\n                #largest magnitude.\n                idx = np.argmax(eigen_val_abs)\n                #Choose the eigenvector corresponding\n                # to the eigenvalue with the \n                # largest magnitude.\n                eigen_vectors = E_obj[1]\n                W = eigen_vectors[:,idx]\n\n            except:\n                #This is a very expensive operation\n                #since it computes all the eigenvectors.\n                if 5000 &lt; n_rows:\n                    print(\"We will use a full eigen decomp.\")\n                    print(f\"The block size is {n_rows}.\")\n                    print(\"Warning ...\")\n                    txt = \"This operation is very expensive.\"\n                    print(txt)\n                inv_row_sums   = 1/row_sums\n                inv_row_sums   = sp.diags(inv_row_sums)\n                laplacian_mtx  = inv_row_sums @ laplacian_mtx\n                eig_obj = np.linalg.eig(laplacian_mtx)\n                eig_vals = eig_obj.eigenvalues\n                eig_vecs = eig_obj.eigenvectors\n                idx = np.argsort(np.abs(np.real(eig_vals)))\n                #Get the index of the second smallest \n                #eigenvalue.\n                idx = idx[1]\n                W = np.real(eig_vecs[:,idx])\n                W = np.squeeze(np.asarray(W))\n\n\n        mask_c1 = 0 &lt; W\n        mask_c2 = ~mask_c1\n\n        #If one partition has all the elements\n        #then return with Q = 0.\n        if mask_c1.all() or mask_c2.all():\n            return (Q, partition)\n\n        masks = [mask_c1, mask_c2]\n\n        for mask in masks:\n            n_rows_msk = mask.sum()\n            partition.append(rows[mask])\n            ones_msk = ones * mask\n            row_sums_msk = S @ ones_msk\n            O_c = ones_msk @ row_sums_msk - n_rows_msk\n            L_c = ones_msk @ row_sums  - n_rows_msk\n            Q += O_c / L - (L_c / L)**2\n\n        if self.verbose_mode:\n            print(f'{Q=}')\n            print(f'I found: {partition=}')\n            print('===========================')\n\n        return (Q, partition)\n\n    #=====================================\n    def store_outputs(\n            self,\n            cell_ann_col: Optional[str] = \"cell_annotations\",\n            ):\n        \"\"\"\n        Store the outputs and plot the branching tree.\n\n        File outputs:\n\n        cluster_list.json: The json file containing a list \n        of clusters. \n\n        cluster_tree.json: The json file containing the \n        output tree in a recursive format.\n\n        graph.dot: A dot file of the tree. It includes the \n        modularity and the size.\n\n        node_info.csv: Size and modularity of each node.\n\n        clusters.csv: The cluster membership for each cell.\n\n        \"\"\"\n\n        self.t0 = clock()\n\n\n        fname = 'graph.dot'\n        dot_fname = os.path.join(self.output, fname)\n\n        nx.nx_agraph.write_dot(self.G, dot_fname)\n        #Write cell to node data frame.\n        self.write_cell_assignment_to_csv()\n        self.convert_graph_to_json()\n        self.write_cluster_list_to_json()\n\n        #Store the cell annotations in the output folder.\n        if 0 &lt; len(cell_ann_col):\n            if cell_ann_col in self.A.obs.columns:\n                self.generate_cell_annotation_file(\n                    cell_ann_col)\n            else:\n                txt = \"Annotation column does not exists.\"\n                #raise ValueError(txt)\n\n        print(self.G)\n\n        #Number of cells for each node\n        size_list = []\n        #Modularity for each node\n        Q_list = []\n        #Node label\n        node_list = []\n\n        for node, attr in self.G.nodes(data=True):\n            node_list.append(node)\n            size_list.append(attr['size'])\n            if 'Q' in attr:\n                Q_list.append(attr['Q'])\n            else:\n                Q_list.append(np.nan)\n\n        #Write node information to CSV\n        D = {'node': node_list, 'size':size_list, 'Q':Q_list}\n        df = pd.DataFrame(D)\n        fname = 'node_info.csv'\n        fname = os.path.join(self.output, fname)\n        df.to_csv(fname, index=False)\n\n        if self.use_twopi_cmd:\n            self.plot_radial_tree_from_dot_file()\n\n        self.tf = clock()\n        delta = self.tf - self.t0\n        txt = ('Elapsed time for storing outputs: ' +\n                f'{delta:.2f} seconds.')\n        print(txt)\n\n\n    #=====================================\n    def convert_mm_from_source_to_anndata(self):\n        \"\"\"\n        This function reads the matrix.mtx file \\\n                located at the source directory.\\\n                Since we assume that the matrix \\\n                has the format genes x cells, we\\\n                transpose the matrix, then \\\n                convert it to the CSR format \\\n                and then into an AnnData object.\n        \"\"\"\n\n        self.t0 = clock()\n\n        print('Loading data from .mtx file.')\n        print('Note that we assume the format:')\n        print('genes=rows and cells=columns.')\n\n        fname = None\n        for f in os.listdir(self.source):\n            if f.endswith('.mtx'):\n                fname = f\n                break\n\n        if fname is None:\n            raise ValueError('.mtx file not found.')\n\n        fname = os.path.join(self.source, fname)\n        mat = mmread(fname)\n        #Remember that the input matrix has\n        #genes for rows and cells for columns.\n        #Thus, just transpose.\n        self.A = mat.T.tocsr()\n\n        fname = 'barcodes.tsv'\n        print(f'Loading {fname}')\n        fname = os.path.join(self.source, fname)\n        df_barcodes = pd.read_csv(\n                fname, delimiter='\\t', header=None)\n        barcodes = df_barcodes.loc[:,0].tolist()\n\n        fname = 'genes.tsv'\n        print(f'Loading {fname}')\n        fname = os.path.join(self.source, fname)\n        df_genes = pd.read_csv(\n                fname, delimiter='\\t', header=None)\n        genes = df_genes.loc[:,0].tolist()\n\n        self.A = AnnData(self.A)\n        self.A.obs_names = barcodes\n        self.A.var_names = genes\n\n        self.tf = clock()\n        delta = self.tf - self.t0\n        txt = ('Elapsed time for loading: ' + \n                f'{delta:.2f} seconds.')\n\n    #=====================================\n    def write_cell_assignment_to_csv(self):\n        \"\"\"\n        This function creates a CSV file that indicates \\\n            the assignment of each cell to a specific \\\n            cluster. The first column is the cell id, \\\n            the second column is the cluster id, and \\\n            the third column is the path from the root \\\n            node to the given node.\n        \"\"\"\n        fname = 'clusters.csv'\n        fname = os.path.join(self.output, fname)\n        labels = ['sp_cluster','sp_path']\n        df = self.A.obs[labels]\n        df.index.names = ['cell']\n        df = df.rename(columns={'sp_cluster':'cluster',\n                                'sp_path':'path'})\n        df.to_csv(fname, index=True)\n\n    #=====================================\n    def write_cluster_list_to_json(self):\n        \"\"\"\n        This function creates a JSON file that indicates \\\n            the assignment of each cell to a specific \\\n            cluster. \n        \"\"\"\n        fname = 'cluster_list.json'\n        fname = os.path.join(self.output, fname)\n        master_list = []\n        relevant_cols = [\"sp_cluster\", \"sp_path\"]\n        df = self.A.obs[relevant_cols]\n        df = df.reset_index(names=\"cell\")\n        df = df.sort_values([\"sp_cluster\",\"cell\"])\n        for idx, row in df.iterrows():\n            cluster = row[\"sp_cluster\"]\n            path_str= row[\"sp_path\"]\n            cell    = row[\"cell\"]\n            nodes = path_str.split(\"/\")\n            list_of_nodes = []\n            sub_dict_1 = {\"unCell\":cell}\n            sub_dict_2 = {\"unRow\":idx}\n            main_dict = {\"_barcode\":sub_dict_1,\n                         \"_cellRow\":sub_dict_2}\n            for node in nodes:\n                d = {\"unCluster\":int(node)}\n                list_of_nodes.append(d)\n\n            master_list.append([main_dict, list_of_nodes])\n\n        s = str(master_list)\n        replace_dict = {' ':'', \"'\":'\"'}\n        pattern = '|'.join(replace_dict.keys())\n        regexp  = re.compile(pattern)\n        fun = lambda x: replace_dict[x.group(0)] \n        obj = regexp.sub(fun, s)\n        with open(fname, 'w') as output_file:\n            output_file.write(obj)\n\n\n    #=====================================\n    def convert_graph_to_json(self):\n        \"\"\"\n        The graph structure stored in the attribute\\\n            self.J has to be formatted into a \\\n            JSON file. This function takes care\\\n            of that task. The output file is \\\n            named 'cluster_tree.json' and is\\\n            equivalent to the 'cluster_tree.json'\\\n            file produced by too-many-cells.\n        \"\"\"\n        fname = \"cluster_tree.json\"\n        fname = os.path.join(self.output, fname)\n        s = str(self.J)\n        replace_dict = {' ':'', 'None':'null', \"'\":'\"'}\n        pattern = '|'.join(replace_dict.keys())\n        regexp  = re.compile(pattern)\n        fun = lambda x: replace_dict[x.group(0)] \n        obj = regexp.sub(fun, s)\n        with open(fname, 'w') as output_file:\n            output_file.write(obj)\n\n    #=====================================\n    def generate_cell_annotation_file(self,\n            cell_ann_col: Optional[str] = \"cell_annotations\",\n            tag: Optional[str]=\"cell_annotation_labels\"\n    ):\n        \"\"\"\n        This function stores a CSV file with\\\n            the labels for each cell.\n\n        :param column: Name of the\\\n            column in the .obs data frame of\\\n            the AnnData object that contains\\\n            the labels to be used for the tree\\\n            visualization. For example, cell \\\n            types.\n\n        \"\"\"\n        if tag[-3:] == \".csv\":\n            pass\n        else:\n            fname = tag + \".csv\"\n\n        fname = os.path.join(self.output, fname)\n        self.cell_annotations_path = fname\n\n        ca = self.A.obs[cell_ann_col].copy()\n        ca.index.names = ['item']\n        ca = ca.rename('label')\n        ca.to_csv(fname, index=True)\n\n    #=====================================\n    def create_data_for_tmci(\n            self,\n            tmci_mtx_dir: Optional[str] = \"tmci_mtx_data\",\n            list_of_genes: Optional[list] = [],\n            path_to_genes: Optional[str] = \"\",\n            create_matrix: Optional[bool] = True,\n            ):\n        \"\"\"\n        Produce the 10X files for a given set of\\\n            genes.  This function produces the\\\n            genes x cells matrix market format matrix,\\\n            the genes.tsv file and the barcodes.\n        If a path is provided for the genes, then the\\\n            first column of the csv file must have the\\\n            gene names.\n        \"\"\"\n\n        self.tmci_mtx_dir = os.path.join(\n            self.output, tmci_mtx_dir)\n\n        os.makedirs(self.tmci_mtx_dir, exist_ok=True)\n\n        # Genes\n        genes_f = \"genes.tsv\"\n        genes_f = os.path.join(self.tmci_mtx_dir, genes_f)\n\n        var_names = []\n        col_indices = []\n\n        if 0 &lt; len(path_to_genes):\n            df = pd.read_csv(path_to_genes, header=0)\n            #The first column should contain the genes.\n            list_of_genes = df.iloc[:,0].to_list()\n\n        if 0 &lt; len(list_of_genes):\n\n            for gene in list_of_genes:\n                if gene not in self.A.var.index:\n                    continue\n                var_names.append(gene)\n                col_index = self.A.var.index.get_loc(gene)\n                col_indices.append(col_index)\n\n            G_mtx = self.A.X[:,col_indices]\n\n        else:\n            #If not list is provided, use all the genes.\n            var_names = self.A.var_names\n            G_mtx = self.A.X\n\n        L = [var_names,var_names]\n        pd.DataFrame(L).transpose().to_csv(\n            genes_f,\n            sep=\"\\t\",\n            header=False,\n            index=False)\n\n        # Barcodes\n        barcodes_f = \"barcodes.tsv\"\n        barcodes_f = os.path.join(self.tmci_mtx_dir,\n                                  barcodes_f)\n        pd.Series(self.A.obs_names).to_csv(\n            barcodes_f,\n            sep=\"\\t\",\n            header=False,\n            index=False)\n\n        # Matrix\n        if create_matrix:\n            matrix_f = \"matrix.mtx\"\n            matrix_f = os.path.join(self.tmci_mtx_dir,\n                                    matrix_f)\n            mmwrite(matrix_f, sp.coo_matrix(G_mtx.T))\n\n    #=====================================\n    def visualize_with_tmc_interactive(self,\n            path_to_tmc_interactive: str,\n            use_column_for_labels: Optional[str] = \"\",\n            port: Optional[int] = 9991,\n            include_matrix_data: Optional[bool] = False,\n            tmci_mtx_dir: Optional[str] = \"\",\n            ) -&gt; None:\n        \"\"\"\n        This function produces a visualization\\\n                using too-many-cells-interactive.\n\n        :param path_to_tmc_interactive: Path to \\\n                the too-many-cells-interactive \\\n                directory.\n        :param use_column_for_labels: Name of the\\\n                column in the .obs data frame of\\\n                the AnnData object that contains\\\n                the labels to be used in the tree\\\n                visualization. For example, cell \\\n                types.\n        :param port: Port to be used to open\\\n                the app in your browser using\\\n                the address localhost:port.\n\n        \"\"\"\n\n        fname = \"cluster_tree.json\"\n        fname = os.path.join(self.output, fname)\n        tree_path = fname\n        port_str = str(port)\n\n\n        bash_exec = \"./start-and-load.sh\"\n\n\n        if len(use_column_for_labels) == 0:\n            label_path_str = \"\"\n            label_path     = \"\"\n        else:\n            self.generate_cell_annotation_file(\n                    use_column_for_labels)\n            label_path_str = \"--label-path\"\n            label_path     = self.cell_annotations_path\n\n        if include_matrix_data:\n            matrix_path_str = \"--matrix-dir\"\n            if 0 &lt; len(tmci_mtx_dir):\n                matrix_dir = tmci_mtx_dir\n            else:\n\n                if len(self.tmci_mtx_dir) == 0:\n                    print(\"No path for TMCI mtx.\")\n                    print(\"Creating TMCI mtx data.\")\n                    self.create_data_for_tmci()\n\n                matrix_dir = self.tmci_mtx_dir\n        else:\n            matrix_path_str = \"\"\n            matrix_dir = \"\"\n\n        command = [\n                bash_exec,\n                matrix_path_str,\n                matrix_dir,\n                '--tree-path',\n                tree_path,\n                label_path_str,\n                label_path,\n                '--port',\n                port_str\n                ]\n\n        command = list(filter(len,command))\n        command = ' '.join(command)\n\n        #Run the command as if we were inside the\n        #too-many-cells-interactive folder.\n        final_command = (f\"(cd {path_to_tmc_interactive} \"\n                f\"&amp;&amp; {command})\")\n        #print(final_command)\n        url = 'localhost:' + port_str\n        txt = (\"Once the app is running, just type in \"\n                f\"your browser \\n        {url}\")\n        print(txt)\n        txt=\"The app will start loading after pressing Enter.\"\n        print(txt)\n        pause = input('Press Enter to continue ...')\n        p = subprocess.call(final_command, shell=True)\n\n    #=====================================\n    def update_cell_annotations(\n            self,\n            df: pd.DataFrame,\n            column: str = \"cell_annotations\"):\n        \"\"\"\n        Insert a column of cell annotations in the \\\n        AnnData.obs data frame. The column in the \\\n        data frame should be called \"label\". The \\\n        name of the column in the AnnData.obs \\\n        data frame is provided by the user through \\\n        the column argument.\n        \"\"\"\n\n        if \"label\" not in df.columns:\n            raise ValueError(\"Missing label column.\")\n\n        #Reindex the data frame.\n        df = df.loc[self.A.obs.index]\n\n        if df.shape[0] != self.A.obs.shape[0]:\n            raise ValueError(\"Data frame size mismatch.\")\n\n        self.A.obs[column] =  df[\"label\"]\n\n    #=====================================\n    def generate_matrix_from_signature_file(\n            self,\n            signature_path: str):\n        \"\"\"\n        Generate a matrix from the signature provided \\\n            through a file. The entries with a positive\n            weight are assumed to be upregulated and \\\n            those with a negative weight are assumed \\\n            to be downregulated. The algorithm will \\\n            standardize the matrix, i.e., centering \\\n            and scaling.\n\n        If the signature has both positive and \\\n            negative weights, two versions will be \\\n            created. The unadjusted version simply \\\n            computes a weighted average using the \\\n            weights provided in the signature file.\\\n            In the adjusted version the weights \\\n            are adjusted to give equal weight to the \\\n            upregulated and downregulated genes.\n\n        Assumptions\n\n        We assume that the file has at least two \\\n            columns. One should be named \"Gene\" and \\\n            the other \"Weight\". \\\n            The count matrix has cells for rows and \\\n            genes for columns.\n        \"\"\"\n\n        df_signature = pd.read_csv(signature_path, header=0)\n\n        Z = sc.pp.scale(self.A, copy=True)\n        Z_is_sparse = sp.issparse(Z)\n\n        vec = np.zeros(Z.X.shape[0])\n\n        up_reg = vec * 0\n        down_reg = vec * 0\n\n        up_count = 0\n        up_weight = 0\n\n        down_count = 0\n        down_weight = 0\n\n        G = df_signature[\"Gene\"]\n        W = df_signature[\"Weight\"]\n\n        for gene, weight in zip(G, W):\n            if gene not in Z.var.index:\n                continue\n            col_index = Z.var.index.get_loc(gene)\n\n            if Z_is_sparse:\n                gene_col = Z.X.getcol(col_index)\n                gene_col = np.squeeze(gene_col.toarray())\n            else:\n                gene_col = Z.X[:,col_index]\n\n            if 0 &lt; weight:\n                up_reg += weight * gene_col\n                up_weight += weight\n                up_count += 1\n            else:\n                down_reg += weight * gene_col\n                down_weight += np.abs(weight)\n                down_count += 1\n\n        total_counts = up_count + down_count\n        total_weight = up_weight + down_weight\n\n        list_of_names = []\n        list_of_gvecs = []\n\n        UnAdjSign = up_reg + down_reg\n        UnAdjSign /= total_weight\n        self.A.obs[\"UnAdjSign\"] = UnAdjSign\n        list_of_gvecs.append(UnAdjSign)\n        list_of_names.append(\"UnAdjSign\")\n\n        up_factor = down_count / total_counts\n        down_factor = up_count / total_counts\n\n        modified_total_counts = 2 * up_count * down_count\n        modified_total_counts /= total_counts\n\n        check = up_factor*up_count + down_factor*down_count\n\n        print(f\"{up_count=}\")\n        print(f\"{down_count=}\")\n        print(f\"{total_counts=}\")\n        print(f\"{modified_total_counts=}\")\n        print(f\"{check=}\")\n        print(f\"{up_factor=}\")\n        print(f\"{down_factor=}\")\n\n\n        mixed_signs = True\n        if 0 &lt; up_count:\n            UpReg   = up_reg / up_count\n            self.A.obs[\"UpReg\"] = UpReg\n            list_of_gvecs.append(UpReg)\n            list_of_names.append(\"UpReg\")\n            print(\"UpRegulated genes: stats\")\n            print(self.A.obs[\"UpReg\"].describe())\n\n        else:\n            mixed_signs = False\n\n        if 0 &lt; down_count:\n            DownReg   = down_reg / down_count\n            self.A.obs[\"DownReg\"] = DownReg\n            list_of_gvecs.append(DownReg)\n            list_of_names.append(\"DownReg\")\n            print(\"DownRegulated genes: stats\")\n            print(self.A.obs[\"DownReg\"].describe())\n            txt = (\"Note: In our representation, \" \n                   \"the higher the value of a downregulated \"\n                   \"gene, the more downregulated it is.\")\n            print(txt)\n        else:\n            mixed_signs = False\n\n        if mixed_signs:\n            AdjSign  = up_factor * up_reg\n            AdjSign += down_factor * down_reg\n            AdjSign /= modified_total_counts\n            self.A.obs[\"AdjSign\"] = AdjSign\n            list_of_gvecs.append(AdjSign)\n            list_of_names.append(\"AdjSign\")\n\n        m = np.vstack(list_of_gvecs)\n\n        #This function will produce the \n        #barcodes.tsv and the genes.tsv file.\n        self.create_data_for_tmci(\n            list_of_genes = list_of_names,\n            create_matrix=False)\n\n\n        m = m.astype(np.float32)\n\n        mtx_path = os.path.join(\n            self.tmci_mtx_dir, \"matrix.mtx\")\n\n        mmwrite(mtx_path, sp.coo_matrix(m))\n\n    #=====================================\n    def load_graph(\n            self,\n            dot_fname: Optional[str]=\"\",\n            ):\n        \"\"\"\n        Load the dot file.\n        \"\"\"\n\n        self.t0 = clock()\n\n\n        if len(dot_fname) == 0:\n            fname = 'graph.dot'\n            dot_fname = os.path.join(self.output, fname)\n\n        if not os.path.exists(dot_fname):\n            raise ValueError(\"File does not exists.\")\n\n        self.G = nx.nx_agraph.read_dot(dot_fname)\n        self.G = nx.DiGraph(self.G)\n        n_nodes = self.G.number_of_nodes()\n\n        # Change string labels to integers.\n        D = {}\n        for k in range(n_nodes):\n            D[str(k)] = k\n\n        self.G = nx.relabel_nodes(self.G, D, copy=True)\n\n        # self.G = nx.convert_node_labels_to_integers(self.G)\n        # Changing the labels to integers using the above \n        # function follows a different numbering scheme to \n        # that given by the labels of the node.\n\n        print(self.G)\n\n    #=====================================\n    def get_path_from_root_to_node(\n            self,\n            target: int,\n            ):\n        \"\"\"\n        For a given node, we find the path from the root \n        to that node.\n        \"\"\"\n\n        node = target\n        path_vec = [node]\n        modularity_vec = [0]\n\n        while node != 0:\n            # Get an iterator for the predecessors.\n            # There should only be one predecessor.\n            predecessors = self.G.predecessors(node)\n            node = next(predecessors)\n            Q = self.G._node[node][\"Q\"]\n            Q = float(Q)\n            path_vec.append(node)\n            modularity_vec.append(Q)\n\n        # We assume that the distance between two children\n        # nodes is equal to the modularity of the parent node.\n        # Hence, the distance from a child to a parent is \n        # half the modularity.\n        modularity_vec = 0.5 * np.array(\n            modularity_vec, dtype=float)\n        path_vec = np.array(path_vec, dtype=int)\n\n        return (path_vec, modularity_vec)\n\n    #=====================================\n    def get_path_from_node_x_to_node_y(\n            self,\n            x: int,\n            y: int,\n            ):\n        \"\"\"\n        For a given pair of nodes x and y, we find the\n        path between those nodes.\n        \"\"\"\n        x_path, x_dist = self.get_path_from_root_to_node(x)\n        y_path, y_dist = self.get_path_from_root_to_node(y)\n\n        x_set = set(x_path)\n        y_set = set(y_path)\n\n        # print(x_dist)\n        # print(y_dist)\n\n        # print(\"===========\")\n\n        # print(x_path)\n        # print(y_path)\n\n        # print(\"===========\")\n\n        intersection = x_set.intersection(y_set)\n        intersection = list(intersection)\n        intersection = np.array(intersection)\n        n_intersection = len(intersection)\n\n        pivot_node = x_path[-n_intersection]\n        pivot_dist = x_dist[-n_intersection]\n\n        x_path = x_path[:-n_intersection]\n        y_path = y_path[:-n_intersection]\n        y_path = y_path[::-1]\n\n        x_dist = x_dist[:-n_intersection]\n        y_dist = y_dist[1:-n_intersection]\n        y_dist = y_dist[::-1]\n\n        full_path = np.hstack((x_path,pivot_node,y_path))\n        full_dist = np.hstack(\n            (x_dist, pivot_dist, pivot_dist, y_dist))\n        full_dist = full_dist.cumsum()\n\n        # print(full_path) \n        # print(full_dist)\n\n        return (full_path, full_dist)\n\n    #=====================================\n    def compute_cluster_mean_expression(\n            self, \n            node: int, \n            genes: Union[list, str],\n            output_list: Optional[bool] = False,\n            ):\n\n        #Get all the descendants for a given node.\n        #This is a set.\n        nodes = nx.descendants(self.G, node)\n\n        if len(nodes) == 0:\n            #This is a leaf node.\n            nodes = [node]\n        else:\n            #Make sure these are leaf nodes.\n            x = self.set_of_leaf_nodes.intersection(nodes)\n            nodes = x\n            nodes = list(nodes)\n\n        is_string = False\n\n        if isinstance(genes, str):\n            is_string = True\n            list_of_genes = [genes]\n        else:\n            list_of_genes = genes\n\n        exp_vec = []\n        mean_exp = 0\n\n        for gene in list_of_genes:\n\n            if gene not in self.A.var.index:\n                raise ValueError(f\"{gene=} was not found.\")\n\n            col_index = self.A.var.index.get_loc(gene)\n\n            mask = self.A.obs[\"sp_cluster\"].isin(nodes)\n            mean_exp = self.A.X[mask, col_index].mean()\n            exp_vec.append(mean_exp)\n\n        # print(f\"{total_exp=}\")\n        # print(f\"{n_cells=}\")\n        # print(f\"{mean_exp=}\")\n\n        if is_string and not output_list:\n            return mean_exp\n        else:\n            return exp_vec\n\n    #=====================================\n    def load_cluster_info(\n            self,\n            cluster_file_path: Optional[str]=\"\",\n            ):\n        \"\"\"\n        Load the cluster file.\n        \"\"\"\n\n        self.t0 = clock()\n\n        if 0 &lt; len(cluster_file_path):\n            cluster_fname = cluster_file_path\n\n        else:\n            fname = 'clusters.csv'\n            cluster_fname = os.path.join(self.output, fname)\n\n        if not os.path.exists(cluster_fname):\n            raise ValueError(\"File does not exists.\")\n\n        df = pd.read_csv(cluster_fname, index_col=0)\n        self.A.obs[\"sp_cluster\"] = df[\"cluster\"]\n\n        self.set_of_leaf_nodes = set(df[\"cluster\"])\n\n\n    #=====================================\n    def plot_expression_from_node_x_to_node_y(\n            self,\n            x: int,\n            y: int,\n            genes: Union[list, str],\n            ):\n        \"\"\"\n        For a given pair of nodes x and y, we compute the \\\n            gene expression path along the path connecting\\\n            those nodes.\n        Make sure that property set_of_leaf_nodes is\\\n            populated with the correct information.\n        \"\"\"\n\n        if isinstance(genes, str):\n            list_of_genes = [genes]\n        else:\n            list_of_genes = genes\n\n        T = self.get_path_from_node_x_to_node_y(x,y)\n        path_vec, dist_vec = T\n        n_nodes = len(path_vec)\n        n_genes = len(list_of_genes)\n        exp_mat = np.zeros((n_genes,n_nodes))\n\n        for col,node in enumerate(path_vec):\n            g_exp = self.compute_cluster_mean_expression(\n                node, list_of_genes)\n            exp_mat[:,col] = g_exp\n\n        fig,ax = plt.subplots()\n\n        # bogus_names = [\"Gene A\", \"Gene B\"]\n        # colors = [\"blue\", \"red\"]\n\n        for row, gene in enumerate(list_of_genes):\n            ax.plot(dist_vec,\n                    exp_mat[row,:],\n                    linewidth=3,\n                    label=gene,\n                    # label=bogus_names[row],\n                    # color = colors[row]\n                    )\n\n        plt.legend()\n        txt = f\"From node {x} to node {y}\"\n        # txt = f\"From node X to node Y\"\n        ax.set_title(txt)\n        ax.set_ylabel(\"Gene expression\")\n        ax.set_xlabel(\"Distance (modularity units)\")\n        plt.ticklabel_format(style='sci',\n                             axis='x',\n                             scilimits=(0,0))\n\n        fname = \"expression_path.pdf\"\n        fname = os.path.join(self.output, fname)\n        fig.savefig(fname, bbox_inches=\"tight\")\n        print(\"Plot has been generated.\")\n\n    #=====================================\n    def plot_radial_tree_from_dot_file(\n            self,\n            dot_fname: Optional[str] = \"\",\n    ):\n        if len(dot_fname) == 0:\n            fname = 'graph.dot'\n            dot_fname = os.path.join(self.output, fname)\n        else:\n            if not os.path.exists(dot_fname):\n                raise ValueError(\"DOT file not found.\")\n\n        fname = 'output_graph.svg'\n        fname = os.path.join(self.output, fname)\n\n        command = ['twopi',\n                '-Groot=0',\n                '-Goverlap=true',\n                '-Granksep=2',\n                '-Tsvg',\n                dot_fname,\n                '&gt;',\n                fname,\n                ]\n        command = ' '.join(command)\n        p = subprocess.call(command, shell=True)\n\n    #=====================================\n    def compute_marker_mean_value_for_cell(\n            self,\n            marker: str,\n            cell: str,\n            cell_ann_col: Optional[str] = \"cell_annotations\",\n    ):\n\n        CA = cell_ann_col\n        if marker not in self.A.var_names:\n            return None\n\n        col_index = self.A.var.index.get_loc(marker)\n        mask = self.A.obs[CA] == cell\n        mean_exp = self.A.X[mask, col_index].mean()\n\n        return mean_exp\n    #=====================================\n    def compute_marker_median_value_for_cell(\n            self,\n            marker: str,\n            cell: str,\n            cell_ann_col: Optional[str] = \"cell_annotations\",\n    ):\n\n        CA = cell_ann_col\n        if marker not in self.A.var_names:\n            return None\n\n        col_index = self.A.var.index.get_loc(marker)\n        mask = self.A.obs[CA] == cell\n        values = self.A.X[mask, col_index].data\n\n        if len(values) == 0:\n            return 0\n\n        return np.median(values)\n\n    #=====================================\n    def compute_mean_expression_from_indices(\n            self,\n            marker: str,\n            indices: list,\n    ):\n\n        if marker not in self.A.var_names:\n            return None\n\n        col_index = self.A.var.index.get_loc(marker)\n        mask = self.A.obs_names.isin(indices)\n        mean_exp = self.A.X[mask, col_index].mean()\n\n        return mean_exp\n\n    #=====================================\n    def compute_median_expression_from_indices(\n            self,\n            marker: str,\n            indices: list,\n    ):\n\n        if marker not in self.A.var_names:\n            return None\n\n        col_index = self.A.var.index.get_loc(marker)\n        mask = self.A.obs_names.isin(indices)\n        values = self.A.X[mask, col_index].data\n\n        if len(values) == 0:\n            return 0\n\n        return np.median(values)\n    #=====================================\n    def find_stable_tree(\n            self,\n            cell_group_path: str,\n            cell_marker_path: str,\n            cell_ann_col: Optional[str] = \"cell_annotations\",\n            clean_threshold: Optional[float] = 0.8,\n            favor_minorities: Optional[bool] = False,\n            conversion_threshold: Optional[float] = 0.9,\n            confirmation_threshold: Optional[float] = 0.9,\n            elimination_ratio: Optional[float] = -1.,\n            homogeneous_leafs: Optional[bool] = False,\n            follow_parent: Optional[bool] = False,\n            follow_majority: Optional[bool] = False,\n            no_mixtures: Optional[bool] = False,\n            storage_path: Optional[str] = \"stable_tree\",\n    ):\n        CA = cell_ann_col\n        tmc_obj = TooManyCells(self, storage_path)\n\n        something_has_changed = False\n        iterations = 0\n\n        while True:\n\n            tmc_obj.annotate_using_tree(\n            cell_group_path,\n            cell_marker_path,\n            cell_ann_col,\n            clean_threshold,\n            favor_minorities,\n            conversion_threshold,\n            confirmation_threshold,\n            elimination_ratio,\n            homogeneous_leafs,\n            follow_parent,\n            follow_majority,\n            no_mixtures,\n            )\n\n            iterations += 1\n\n            if not tmc_obj.labels_have_changed:\n                #No cells have changed their label\n                #and no cell has been tagged for \n                #elimination.\n                print(\"Nothing has changed.\")\n                break\n\n            something_has_changed = True\n\n            #We know the labels have changed.\n            #We will only recompute the tree if \n            #cells have been eliminated.\n\n            S = tmc_obj.cells_to_be_eliminated\n\n            if 0 == len(S):\n                print(\"No cells have been eliminated.\")\n                break\n\n            #Cells have been eliminated.\n            #A new tree will be generated.\n            mask = tmc_obj.A.obs_names.isin(S)\n            A = tmc_obj.A[~mask].copy()\n            tmc_obj = TooManyCells(A, storage_path)\n            tmc_obj.run_spectral_clustering()\n            tmc_obj.store_outputs()\n\n        if something_has_changed:\n            print(f\"{iterations=}\")\n\n\n    #=====================================\n    def check_leaf_homogeneity(\n            self,\n            cell_ann_col: Optional[str] = \"cell_annotations\",\n    ):\n\n        CA = cell_ann_col\n\n        for node in self.G.nodes:\n            if 0 &lt; self.G.out_degree(node):\n                continue\n\n            #Child\n            mask = self.A.obs[\"sp_cluster\"].isin([node])\n            S = self.A.obs[CA].loc[mask].unique()\n\n            if len(S) == 1:\n                #The node is already homogeneous\n                continue\n            else:\n                #We found one leaf node that is not\n                #homogeneous.\n                self.leaf_nodes_are_homogeneous = False\n                return False\n\n        self.leaf_nodes_are_homogeneous = True\n\n        return True\n\n    #=====================================\n    def homogenize_leaf_nodes(\n            self,\n            cell_ann_col: Optional[str] = \"cell_annotations\",\n            follow_parent: Optional[bool] = False,\n            follow_majority: Optional[bool] = False,\n    ):\n\n        if follow_parent == follow_majority:\n            print(\"Homogeneous leafs strategy:\")\n            raise ValueError(\"Strategy has to be unique.\")\n\n        CA = cell_ann_col\n        elim_set = set()\n\n        for node in self.G.nodes:\n            if 0 &lt; self.G.out_degree(node):\n                continue\n            parent = next(self.G.predecessors(node))\n            #print(f\"{parent}--&gt;{node}\")\n\n            #Child\n            mask = self.A.obs[\"sp_cluster\"].isin([node])\n            S = self.A.obs[CA].loc[mask]\n            vc = S.value_counts(normalize=True)\n            child_majority = vc.index[0]\n            child_ratio = vc.iloc[0]\n\n            if child_ratio == 1:\n                #The node is already homogeneous\n                continue\n\n            if follow_parent:\n                #Parent\n                mask = self.A.obs[\"sp_cluster\"].isin([parent])\n                S = self.A.obs[CA].loc[mask]\n                vc = S.value_counts(normalize=True)\n                parent_majority = vc.index[0]\n\n                #Who is different from the parent?\n                mask = S != parent_majority\n                Q = S.loc[mask]\n                elim_set.update(Q.index)\n                continue\n\n            if follow_majority:\n                #Who is different from the child's majority?\n                mask = S != child_majority\n                Q = S.loc[mask]\n                elim_set.update(Q.index)\n                continue\n\n        return elim_set\n\n\n    #=====================================\n    def erase_cells_from_json_file(\n            self,\n            json_file_path: Optional[str] = \"\",\n            target_json_file_path: Optional[str] = \"\",\n    ):\n\n        #{'_barcode':\n        #{'unCell': 'CAGCTGGCACGGTAGA-176476-OM'},\n        #'_cellRow': {'unRow': 29978}}\n        if len(json_file_path) == 0:\n            folder = \"tmc_outputs\"\n            source = os.getcwd()\n            fname = \"cluster_tree.json\"\n            source_fname = os.path.join(\n                source, folder, fname)\n            fname = \"pruned_cluster_tree.json\"\n            target_fname = os.path.join(\n                source, folder, fname)\n\n        else:\n            source_fname = json_file_path\n            target_fname = target_json_file_path\n\n        with open(source_fname, \"r\") as f:\n            source = f.readline()\n\n        list_of_regexp = []\n        replace_dict = {}\n\n        for k, barcode in enumerate(\n            self.cells_to_be_eliminated):\n\n            txt = '[{][^{]+[{][a-zA-Z\":]+[ ]?[\"]'\n            # txt += \"(?P&lt;barcode&gt;\"\n            txt += barcode\n            # txt += \")\"\n            txt += '[\"][}][^}]+[}]{2}[,]?'\n            list_of_regexp.append(txt)\n            replace_dict[barcode] = \"\"\n\n        pattern = \"|\".join(list_of_regexp)\n        regexp = re.compile(pattern)\n        fun = lambda x: \"\"\n        obj = regexp.sub(fun, source)\n\n        with open(target_fname, \"w\") as output_file:\n            output_file.write(obj)\n\n\n    #=================================================\n    def check_if_cells_belong_to_group(\n            self,\n            cells: pd.Series,\n            group: str,\n            conversion_threshold: Optional[float] = 0.9,\n            cell_ann_col: Optional[str] = \"cell_annotations\",\n    ):\n        \"\"\"\n        The cells parameter is a series that contains\n        the cells types as values and the indices \n        correspond to the barcodes.\n        \"\"\"\n        #This is the question we are trying to\n        #answer.\n        belongs_to_group = False\n        CA = cell_ann_col\n\n        #What cells types belong to \n        #the given group?\n        x = self.group_to_cell_types[group]\n        cell_types_in_group = x\n\n        #Now we are going to iterate over the\n        #cells that belong to the majority\n        #group. We do this to determine if \n        #the non-majority cells could qualify\n        #as a member of the majority group by\n        #using a marker for cells of the \n        #majority group.\n        for cell_type in cell_types_in_group:\n            if belongs_to_group:\n                break\n            print(f\"Are they {cell_type}?\")\n            markers = self.cell_type_to_markers[cell_type]\n\n            for marker in markers:\n                x = self.marker_to_median_value[marker]\n                marker_value = x\n                if marker_value is None:\n                    #Nothing to be done.\n                    continue\n\n                x=self.compute_median_expression_from_indices(\n                    marker, cells.index)\n                expression_value = x\n                print(\"\\t\", marker, marker_value, x)\n                #Let X be the mean/median expression \n                #value of that marker for the\n                #given minority.\n                #Let Y be the mean/median expression\n                #value of that same marker for\n                #the cells in the sample that \n                #are known to express that\n                #marker. If X is above Y \n                #multiplied by the conversion\n                #threshold, then we add that\n                #minority to the majority,\n                if marker_value * conversion_threshold &lt; x:\n                    belongs_to_group = True\n                    print(\"\\t\",\"To convert.\")\n                    break\n\n        if belongs_to_group:\n            self.A.obs[CA].loc[cells.index] = group\n            return True\n        else:\n            print(\"===============================\")\n            print(f\"&gt;&gt;&gt;Cells do not belong to {group}.\")\n            print(\"===============================\")\n            return False\n\n\n\n\n    #=====================================\n    def annotate_using_tree(\n            self,\n            cell_group_path: str,\n            cell_marker_path: str,\n            cell_ann_col: Optional[str] = \"cell_annotations\",\n            clean_threshold: Optional[float] = 0.8,\n            favor_minorities: Optional[bool] = False,\n            conversion_threshold: Optional[float] = 0.9,\n            confirmation_threshold: Optional[float] = 0.9,\n            elimination_ratio: Optional[float] = -1.,\n            homogeneous_leafs: Optional[bool] = False,\n            follow_parent: Optional[bool] = False,\n            follow_majority: Optional[bool] = False,\n            no_mixtures: Optional[bool] = False,\n    ):\n        if not os.path.exists(cell_group_path):\n            print(cell_group_path)\n            raise ValueError(\"File does not exists.\")\n\n        if not os.path.exists(cell_marker_path):\n            print(cell_marker_path)\n            raise ValueError(\"File does not exists.\")\n\n        if homogeneous_leafs:\n            if follow_majority == follow_parent:\n                print(\"Homogeneous leafs strategy:\")\n                raise ValueError(\"Strategy is not unique.\")\n\n        df_cg = pd.read_csv(cell_group_path)\n        print(\"===============================\")\n        print(\"Cell to Group file\")\n        print(df_cg)\n        CA = cell_ann_col\n\n        df_cm = pd.read_csv(cell_marker_path)\n        print(\"===============================\")\n        print(\"Cell to Marker file\")\n        print(df_cm)\n\n        cell_to_group = {}\n        cell_types_to_erase = []\n        self.group_to_cell_types = defaultdict(list)\n\n        self.cells_to_be_eliminated = None\n\n        #Create the cell to group dictionary and\n        #the group to cell dictionary\n        for index, row in df_cg.iterrows():\n            cell = row[\"Cell\"]\n            group = row[\"Group\"]\n\n            if pd.isna(group):\n                group = cell\n            elif group == \"0\":\n                cell_types_to_erase.append(cell)\n                continue\n\n            cell_to_group[cell] = group\n            self.group_to_cell_types[group].append(cell)\n\n\n        #Create the cell to markers dictionary and\n        #marker to value dictionary.\n        self.cell_type_to_markers = defaultdict(list)\n        marker_to_mean_value = {}\n        self.marker_to_median_value = {}\n\n        for index, row in df_cm.iterrows():\n\n            cell = row[\"Cell\"]\n            marker = row[\"Marker\"]\n            self.cell_type_to_markers[cell].append(marker)\n\n            if cell not in cell_to_group:\n                continue\n\n            if marker not in marker_to_mean_value:\n                x = self.compute_marker_median_value_for_cell(\n                    marker, cell)\n                self.marker_to_median_value[marker] = x\n\n        #Eliminate cells that belong to the erase category.\n        if 0 &lt; len(cell_types_to_erase):\n            mask = self.A.obs[CA].isin(cell_types_to_erase)\n            n_cells = mask.sum()\n            vc = self.A.obs[CA].loc[mask].value_counts()\n            #Take the complement of the cells we \n            #want to erase.\n            self.A = self.A[~mask].copy()\n            print(\"===============================\")\n            print(f\"{n_cells} cells have been deleted.\")\n            print(vc)\n\n        #Create a series where the original cell \n        #annotations have been mapped to their \n        #corresponding group.\n        S = self.A.obs[CA].astype(str)\n        for cell, group in cell_to_group.items():\n\n            if cell == group:\n                continue\n\n            mask = S == cell\n            S.loc[mask] = group\n\n        S = S.astype(\"category\")\n        OCA = \"original_cell_annotations\"\n        self.A.obs[OCA] = self.A.obs[CA]\n        self.A.obs[CA] = S\n        vc = self.A.obs[CA].value_counts()\n        print(\"===============================\")\n        print(\"Relabeled cell counts\")\n        print(vc)\n\n        node = 0\n        parent_majority = None\n        parent_ratio = None\n        # We use a deque to do a breadth-first traversal.\n        DQ = deque()\n\n        T = (node, parent_majority, parent_ratio)\n        DQ.append(T)\n\n        iteration = 0\n\n        # Elimination container\n        elim_set = set()\n\n        self.labels_have_changed = False\n\n        while 0 &lt; len(DQ):\n            print(\"===============================\")\n            T = DQ.popleft()\n            node, parent_majority, parent_ratio = T\n            children = self.G.successors(node)\n            nodes = nx.descendants(self.G, node)\n            is_leaf_node = False\n            if len(nodes) == 0:\n                is_leaf_node = True\n                nodes = [node]\n            else:\n                x = self.set_of_leaf_nodes.intersection(\n                    nodes)\n                nodes = list(x)\n\n            mask = self.A.obs[\"sp_cluster\"].isin(nodes)\n            S = self.A.obs[CA].loc[mask]\n            node_size = mask.sum()\n            print(f\"Working with {node=}\")\n            print(f\"Size of {node=}: {node_size}\")\n            vc = S.value_counts(normalize=True)\n            print(\"===============================\")\n            print(vc)\n\n            majority_group = vc.index[0]\n            majority_ratio = vc.iloc[0]\n\n            if majority_ratio == 1:\n                #The cluster is homogeneous.\n                #Nothing to do here.\n                continue\n\n\n            if majority_ratio &lt; clean_threshold:\n                #We are below clean_threshold, so we add \n                #these nodes to the deque for \n                #further processing.\n                print(\"===============================\")\n                for child in children:\n                    print(f\"Adding node {child} to DQ.\")\n                    T = (child,\n                         majority_group,\n                         majority_ratio)\n                    DQ.append(T)\n            else:\n                #We are above the cleaning threshold. \n                #Hence, we can star cleaning this node.\n                print(\"===============================\")\n                print(f\"Cleaning {node=}.\")\n                print(f\"{majority_group=}.\")\n                print(f\"{majority_ratio=}.\")\n\n                if no_mixtures:\n                    #We do not allow minorities.\n                    mask = S != majority_group\n                    Q = S.loc[mask]\n                    elim_set.update(Q.index)\n                    continue\n\n                #We are going to iterate over all the \n                #groups below the majority group.\n                #We call these the minority_groups.\n\n                #We have two options. Start checking if\n                #the minority actually belongs to the \n                #majority or first check if the minority\n                #is indeed a true minority.\n                for minority_group, mr in vc.iloc[1:].items():\n\n                    minority_ratio = mr\n\n                    #These are the cells that belong to one\n                    #of the minorities. We label them as\n                    #Q because their current status \n                    #is under question.\n                    mask = S == minority_group\n                    minority_size = mask.sum()\n                    if minority_size == 0:\n                        #Nothing to be done with this and \n                        #subsequent minorities because the\n                        #cell ratios are sorted in \n                        #decreasing order. If one is zero,\n                        #the rest are zero too.\n                        break\n                    Q = S.loc[mask]\n\n                    if minority_ratio &lt; elimination_ratio:\n                        #If the ratio is below the \n                        #given threshold, then we \n                        #remove these cells.\n                        elim_set.update(Q.index)\n                        continue\n\n                    #Check membership\n                    if favor_minorities:\n                        #We first check if the minority is\n                        #indeed a true minority.\n                        x=self.check_if_cells_belong_to_group(\n                            Q, \n                            minority_group, \n                            conversion_threshold,\n                            cell_ann_col,\n                        )\n                        belongs_to_minority = x\n                        if belongs_to_minority:\n                            #Move to the next minority.\n                            continue\n                        #Otherwise, check if belongs to \n                        #the majority group.\n                        x=self.check_if_cells_belong_to_group(\n                            Q, \n                            majority_group, \n                            conversion_threshold,\n                            cell_ann_col,\n                        )\n                        identity_was_determined = x\n                        belongs_to_majority = x\n\n                        if belongs_to_majority:\n                            self.labels_have_changed = True\n\n                    else:\n                        #We first check if the minority is\n                        #actually part of the majority.\n                        x=self.check_if_cells_belong_to_group(\n                            Q, \n                            majority_group, \n                            conversion_threshold,\n                            cell_ann_col,\n                        )\n                        belongs_to_majority = x\n                        if belongs_to_majority:\n                            #Move to the next minority.\n                            self.labels_have_changed = True\n                            continue\n                        #Otherwise, check if belongs to \n                        #the minority group.\n                        x=self.check_if_cells_belong_to_group(\n                            Q, \n                            minority_group, \n                            conversion_threshold,\n                            cell_ann_col,\n                        )\n                        identity_was_determined = x\n\n                    if identity_was_determined:\n                        #Nothing to be done.\n                        #Move to the next minority.\n                        continue\n                    else:\n                        #Cells could not be classified\n                        #and therefore will be eliminated.\n                        elim_set.update(Q.index)\n\n\n            if iteration == 1:\n                pass\n                #break\n            else:\n                iteration += 1\n\n\n        #Elimination phase 1\n        print(\"Elimination set size before homogenization:\",\n              len(elim_set))\n\n        #Homogenization\n        if homogeneous_leafs:\n\n            if follow_parent:\n                print(\"Using parent node majority.\")\n\n            if follow_majority:\n                print(\"Using leaf node majority.\")\n\n            S = self.homogenize_leaf_nodes(\n                CA,\n                follow_parent,\n                follow_majority)\n\n            if 0 &lt; len(S):\n                print(\"Cells lost through homogenization:\",\n                    len(S))\n                elim_set.update(S)\n\n        if 0 &lt; len(elim_set):\n            print(\"Total cells lost:\", len(elim_set))\n            remaining_cells = self.A.X.shape[0]\n            remaining_cells -= len(elim_set)\n            print(\"Remaining cells:\", remaining_cells)\n\n            #Create a new category.\n            x = self.A.obs[CA].cat.add_categories(\"X\")\n            self.A.obs[CA] = x\n            #Label the cells to be eliminated with \"X\".\n            mask = self.A.obs_names.isin(elim_set)\n            self.A.obs[CA].loc[mask] = \"X\"\n\n            self.labels_have_changed = True\n\n        if self.labels_have_changed:\n            self.generate_cell_annotation_file(\n                cell_ann_col=CA, tag = \"updated_cell_labels\")\n        else:\n            print(\"Nothing has changed.\")\n\n        self.cells_to_be_eliminated = elim_set\n\n\n    #====END=OF=CLASS=====================\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.__init__","title":"<code>__init__(self, input, output='', input_is_matrix_market=False, use_full_matrix=False)</code>  <code>special</code>","text":"<p>The constructor takes the following inputs.</p> <p>:param input: Path to input directory or                 AnnData object. :param output: Path to output directory. :param input_is_matrix_market: If true,                 the directory should contain a                 .mtx file, a barcodes.tsv file                 and a genes.tsv file.</p> <p>:return: a TooManyCells object. :rtype: :obj:<code>TooManyCells</code></p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def __init__(self,\n        input: Union[AnnData, str],\n        output: Optional[str] = \"\",\n        input_is_matrix_market: Optional[bool] = False,\n        use_full_matrix: Optional[bool] = False,\n        ):\n    \"\"\"\n    The constructor takes the following inputs.\n\n    :param input: Path to input directory or \\\n            AnnData object.\n    :param output: Path to output directory.\n    :param input_is_matrix_market: If true, \\\n            the directory should contain a \\\n            .mtx file, a barcodes.tsv file \\\n            and a genes.tsv file.\n\n    :return: a TooManyCells object.\n    :rtype: :obj:`TooManyCells`\n\n    \"\"\"\n\n    #We use a directed graph to enforce the parent\n    #to child relation.\n    self.G = nx.DiGraph()\n\n    self.set_of_leaf_nodes = set()\n\n    if isinstance(input, TooManyCells):\n\n        #Clone the given TooManyCells object.\n        self.A = input.A.copy()\n        self.G = input.G.copy()\n        S = input.set_of_leaf_nodes.copy()\n        self.set_of_leaf_nodes = S\n\n    elif isinstance(input, str):\n        self.source = os.path.abspath(input)\n        if self.source.endswith('.h5ad'):\n            self.t0 = clock()\n            self.A = ad.read_h5ad(self.source)\n            self.tf = clock()\n            delta = self.tf - self.t0\n            txt = ('Elapsed time for loading: ' +\n                    f'{delta:.2f} seconds.')\n            print(txt)\n        else:\n            if input_is_matrix_market:\n                self.convert_mm_from_source_to_anndata()\n            else:\n                for f in os.listdir(self.source):\n                    if f.endswith('.h5ad'):\n                        fname = os.path.join(\n                            self.source, f)\n                        self.t0 = clock()\n                        self.A = ad.read_h5ad(fname)\n                        self.tf = clock()\n                        delta = self.tf - self.t0\n                        txt = ('Elapsed time for ' +\n                               'loading: ' +\n                                f'{delta:.2f} seconds.')\n                        print(txt)\n                        break\n\n    elif isinstance(input, AnnData):\n        self.A = input\n    else:\n        raise ValueError('Unexpected input type.')\n\n    #If no output directory is provided,\n    #we use the current working directory.\n    if output == \"\":\n        output = os.getcwd()\n        output = os.path.join(output, \"tmc_outputs\")\n        print(f\"Outputs will be saved in: {output}\")\n\n    if not os.path.exists(output):\n        os.makedirs(output)\n\n    self.output = os.path.abspath(output)\n\n    #This column of the obs data frame indicates\n    #the correspondence between a cell and the \n    #leaf node of the spectral clustering tree.\n    sp_cluster = \"sp_cluster\"\n    sp_path = \"sp_path\"\n    if sp_cluster not in self.A.obs.columns:\n        self.A.obs[sp_cluster] = -1\n    if sp_path not in self.A.obs.columns:\n        self.A.obs[sp_path]    = \"\"\n\n    t = self.A.obs.columns.get_loc(sp_cluster)\n    self.cluster_column_index = t\n    t = self.A.obs.columns.get_loc(sp_path)\n    self.path_column_index = t\n\n    self.delta_clustering = 0\n    self.final_n_iter     = 0\n\n    #Create a copy to avoid direct modifications\n    #of the original count matrix X.\n    #Note that we are making sure that the \n    #sparse matrix has the CSR format. This\n    #is relevant when we normalize.\n    if sp.issparse(self.A.X):\n        #Compute the density of the matrix\n        rho = self.A.X.nnz / np.prod(self.A.X.shape)\n        #If more than 50% of the matrix is occupied,\n        #we generate a dense version of the matrix.\n        sparse_threshold = 0.50\n        if use_full_matrix or sparse_threshold &lt; rho:\n            self.is_sparse = False\n            self.X = self.A.X.toarray()\n            txt = (\"Using a dense representation\" \n                   \" of the count matrix.\")\n            print(txt)\n            txt = (\"Values will be converted to\" \n                   \" float32.\")\n            print(txt)\n            self.X = self.X.astype(np.float32)\n        else:\n            self.is_sparse = True\n            #Make sure we use a CSR format.\n            self.X = sp.csr_matrix(self.A.X,\n                                   dtype=np.float32,\n                                   copy=True)\n    else:\n        #The matrix is dense.\n        print(\"The matrix is dense.\")\n        self.is_sparse = False\n        self.X = self.A.X.copy()\n        txt = (\"Values will be converted to\" \n               \" float32.\")\n        print(txt)\n        self.X = self.X.astype(np.float32)\n\n    self.n_cells, self.n_genes = self.A.shape\n\n    if self.n_cells &lt; 3:\n        raise ValueError(\"Too few observations (cells).\")\n\n    print(self.A)\n\n    #Location of the matrix data for TMCI\n    self.tmci_mtx_dir = \"\"\n\n    self.spectral_clustering_has_been_called = False\n\n    self.cells_to_be_eliminated = None\n\n    x = \"/home/javier/Documents/repos/too-many-cells-interactive\"\n\n    # We use a deque to offer the possibility of breadth-\n    # versus depth-first. Our current implementation\n    # uses depth-first to be consistent with the \n    # numbering scheme of TooManyCellsInteractive.\n    self.DQ = deque()\n\n\n    #Map a node to the path in the\n    #binary tree that connects the\n    #root node to the given node.\n    self.node_to_path = {}\n\n    #Map a node to a list of indices\n    #that provide access to the JSON\n    #structure.\n    self.node_to_j_index = {}\n\n    #the JSON structure representation\n    #of the tree.\n    self.J = MultiIndexList()\n\n    self.node_counter = 0\n\n    #The threshold for modularity to \n    #accept a given partition of a set\n    #of cells.\n    self.eps = 1e-9\n\n    self.use_twopi_cmd   = True\n    self.verbose_mode    = False\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.check_if_cells_belong_to_group","title":"<code>check_if_cells_belong_to_group(self, cells, group, conversion_threshold=0.9, cell_ann_col='cell_annotations')</code>","text":"<p>The cells parameter is a series that contains the cells types as values and the indices  correspond to the barcodes.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def check_if_cells_belong_to_group(\n        self,\n        cells: pd.Series,\n        group: str,\n        conversion_threshold: Optional[float] = 0.9,\n        cell_ann_col: Optional[str] = \"cell_annotations\",\n):\n    \"\"\"\n    The cells parameter is a series that contains\n    the cells types as values and the indices \n    correspond to the barcodes.\n    \"\"\"\n    #This is the question we are trying to\n    #answer.\n    belongs_to_group = False\n    CA = cell_ann_col\n\n    #What cells types belong to \n    #the given group?\n    x = self.group_to_cell_types[group]\n    cell_types_in_group = x\n\n    #Now we are going to iterate over the\n    #cells that belong to the majority\n    #group. We do this to determine if \n    #the non-majority cells could qualify\n    #as a member of the majority group by\n    #using a marker for cells of the \n    #majority group.\n    for cell_type in cell_types_in_group:\n        if belongs_to_group:\n            break\n        print(f\"Are they {cell_type}?\")\n        markers = self.cell_type_to_markers[cell_type]\n\n        for marker in markers:\n            x = self.marker_to_median_value[marker]\n            marker_value = x\n            if marker_value is None:\n                #Nothing to be done.\n                continue\n\n            x=self.compute_median_expression_from_indices(\n                marker, cells.index)\n            expression_value = x\n            print(\"\\t\", marker, marker_value, x)\n            #Let X be the mean/median expression \n            #value of that marker for the\n            #given minority.\n            #Let Y be the mean/median expression\n            #value of that same marker for\n            #the cells in the sample that \n            #are known to express that\n            #marker. If X is above Y \n            #multiplied by the conversion\n            #threshold, then we add that\n            #minority to the majority,\n            if marker_value * conversion_threshold &lt; x:\n                belongs_to_group = True\n                print(\"\\t\",\"To convert.\")\n                break\n\n    if belongs_to_group:\n        self.A.obs[CA].loc[cells.index] = group\n        return True\n    else:\n        print(\"===============================\")\n        print(f\"&gt;&gt;&gt;Cells do not belong to {group}.\")\n        print(\"===============================\")\n        return False\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.compute_partition_for_gen","title":"<code>compute_partition_for_gen(self, rows)</code>","text":"<p>Compute the partition of the given set            of cells. The rows input             contains the indices of the             rows we are to partition.             The algorithm computes a truncated             SVD and the corresponding modularity             of the newly created communities.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def compute_partition_for_gen(self, rows: np.ndarray\n) -&gt; tuple:\n#) -&gt; tuple[float, np.ndarray]:\n    \"\"\"\n    Compute the partition of the given set\\\n        of cells. The rows input \\\n        contains the indices of the \\\n        rows we are to partition. \\\n        The algorithm computes a truncated \\\n        SVD and the corresponding modularity \\\n        of the newly created communities.\n    \"\"\"\n\n    if self.verbose_mode:\n        print(f'I was given: {rows=}')\n\n    partition = []\n    Q = 0\n\n    n_rows = len(rows) \n    #print(f\"Number of cells: {n_rows}\")\n\n    #If the number of rows is less than 3,\n    #we keep the cluster as it is.\n    if n_rows &lt; 3:\n        return (Q, partition)\n\n    S = self.X[np.ix_(rows, rows)]\n    ones = np.ones(n_rows)\n    row_sums = S.dot(ones)\n    row_sums_mtx   = sp.diags(row_sums)\n    laplacian_mtx  = row_sums_mtx - S\n    L = np.sum(row_sums) - n_rows\n\n    zero_row_sums_mask = np.abs(row_sums) &lt; self.eps\n    has_zero_row_sums = zero_row_sums_mask.any()\n    has_neg_row_sums = (row_sums &lt; -self.eps).any() \n\n    if has_zero_row_sums:\n        print(\"We have zero row sums.\")\n        row_sums[zero_row_sums_mask] = 0\n\n    if has_neg_row_sums and has_zero_row_sums:\n        txt = \"This matrix cannot be processed.\"\n        print(txt)\n        txt = \"Cannot have negative and zero row sums.\"\n        raise ValueError(txt)\n\n    if has_neg_row_sums:\n        #This is a very expensive operation\n        #since it computes all the eigenvectors.\n        if 5000 &lt; n_rows:\n            print(\"The row sums are negative.\")\n            print(\"We will use a full eigen decomp.\")\n            print(f\"The block size is {n_rows}.\")\n            print(\"Warning ...\")\n            txt = \"This operation is very expensive.\"\n            print(txt)\n        inv_row_sums   = 1/row_sums\n        inv_row_sums   = sp.diags(inv_row_sums)\n        laplacian_mtx  = inv_row_sums @ laplacian_mtx\n        eig_obj = np.linalg.eig(laplacian_mtx)\n        eig_vals = eig_obj.eigenvalues\n        eig_vecs = eig_obj.eigenvectors\n        idx = np.argsort(np.abs(np.real(eig_vals)))\n        idx = idx[1]\n        W = np.real(eig_vecs[:,idx])\n        W = np.squeeze(np.asarray(W))\n\n    else:\n        #Nonnegative row sums.\n        try:\n            E_obj = Eigen_Hermitian(laplacian_mtx,\n                                    k=2,\n                                    M=row_sums_mtx,\n                                    sigma=0,\n                                    which=\"LM\")\n            eigen_val_abs = np.abs(E_obj[0])\n            #Identify the eigenvalue with the\n            #largest magnitude.\n            idx = np.argmax(eigen_val_abs)\n            #Choose the eigenvector corresponding\n            # to the eigenvalue with the \n            # largest magnitude.\n            eigen_vectors = E_obj[1]\n            W = eigen_vectors[:,idx]\n\n        except:\n            #This is a very expensive operation\n            #since it computes all the eigenvectors.\n            if 5000 &lt; n_rows:\n                print(\"We will use a full eigen decomp.\")\n                print(f\"The block size is {n_rows}.\")\n                print(\"Warning ...\")\n                txt = \"This operation is very expensive.\"\n                print(txt)\n            inv_row_sums   = 1/row_sums\n            inv_row_sums   = sp.diags(inv_row_sums)\n            laplacian_mtx  = inv_row_sums @ laplacian_mtx\n            eig_obj = np.linalg.eig(laplacian_mtx)\n            eig_vals = eig_obj.eigenvalues\n            eig_vecs = eig_obj.eigenvectors\n            idx = np.argsort(np.abs(np.real(eig_vals)))\n            #Get the index of the second smallest \n            #eigenvalue.\n            idx = idx[1]\n            W = np.real(eig_vecs[:,idx])\n            W = np.squeeze(np.asarray(W))\n\n\n    mask_c1 = 0 &lt; W\n    mask_c2 = ~mask_c1\n\n    #If one partition has all the elements\n    #then return with Q = 0.\n    if mask_c1.all() or mask_c2.all():\n        return (Q, partition)\n\n    masks = [mask_c1, mask_c2]\n\n    for mask in masks:\n        n_rows_msk = mask.sum()\n        partition.append(rows[mask])\n        ones_msk = ones * mask\n        row_sums_msk = S @ ones_msk\n        O_c = ones_msk @ row_sums_msk - n_rows_msk\n        L_c = ones_msk @ row_sums  - n_rows_msk\n        Q += O_c / L - (L_c / L)**2\n\n    if self.verbose_mode:\n        print(f'{Q=}')\n        print(f'I found: {partition=}')\n        print('===========================')\n\n    return (Q, partition)\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.compute_partition_for_sp","title":"<code>compute_partition_for_sp(self, rows)</code>","text":"<p>Compute the partition of the given set            of cells. The rows input             contains the indices of the             rows we are to partition.             The algorithm computes a truncated             SVD and the corresponding modularity             of the newly created communities.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def compute_partition_for_sp(self, rows: np.ndarray\n) -&gt; tuple:\n#) -&gt; tuple[float, np.ndarray]:\n    \"\"\"\n    Compute the partition of the given set\\\n        of cells. The rows input \\\n        contains the indices of the \\\n        rows we are to partition. \\\n        The algorithm computes a truncated \\\n        SVD and the corresponding modularity \\\n        of the newly created communities.\n    \"\"\"\n\n    if self.verbose_mode:\n        print(f'I was given: {rows=}')\n\n    partition = []\n    Q = 0\n\n    n_rows = len(rows) \n    #print(f\"Number of cells: {n_rows}\")\n\n    #If the number of rows is less than 3,\n    #we keep the cluster as it is.\n    if n_rows &lt; 3:\n        return (Q, partition)\n\n    B = self.X[rows,:]\n    ones = np.ones(n_rows)\n    partial_row_sums = B.T.dot(ones)\n    #1^T @ B @ B^T @ 1 = (B^T @ 1)^T @ (B^T @ 1)\n    L = partial_row_sums @ partial_row_sums - n_rows\n    #These are the row sums of the similarity matrix\n    row_sums = B @ partial_row_sums\n    #Check if we have negative entries before computing\n    #the square root.\n    # if  neg_row_sums or self.use_eig_decomp:\n    zero_row_sums_mask = np.abs(row_sums) &lt; self.eps\n    has_zero_row_sums = zero_row_sums_mask.any()\n    has_neg_row_sums = (row_sums &lt; -self.eps).any() \n\n    if has_zero_row_sums:\n        print(\"We have zero row sums.\")\n        row_sums[zero_row_sums_mask] = 0\n\n    if has_neg_row_sums and has_zero_row_sums:\n        txt = \"This matrix cannot be processed.\"\n        print(txt)\n        txt = \"Cannot have negative and zero row sums.\"\n        raise ValueError(txt)\n\n    if  has_neg_row_sums:\n        #This means we cannot use the fast approach\n        #We'll have to build a dense representation\n        # of the similarity matrix.\n        if 5000 &lt; n_rows:\n            print(\"The row sums are negative.\")\n            print(\"We will use a full eigen decomp.\")\n            print(f\"The block size is {n_rows}.\")\n            print(\"Warning ...\")\n            txt = \"This operation is very expensive.\"\n            print(txt)\n        laplacian_mtx  = B @ B.T\n        row_sums_mtx   = sp.diags(row_sums)\n        laplacian_mtx  = row_sums_mtx - laplacian_mtx\n\n        #This is a very expensive operation\n        #since it computes all the eigenvectors.\n        inv_row_sums   = 1/row_sums\n        inv_row_sums   = sp.diags(inv_row_sums)\n        laplacian_mtx  = inv_row_sums @ laplacian_mtx\n        eig_obj = np.linalg.eig(laplacian_mtx)\n        eig_vals = eig_obj.eigenvalues\n        eig_vecs = eig_obj.eigenvectors\n        idx = np.argsort(np.abs(np.real(eig_vals)))\n        #Get the index of the second smallest eigenvalue.\n        idx = idx[1]\n        W = np.real(eig_vecs[:,idx])\n        W = np.squeeze(np.asarray(W))\n\n    elif self.use_eig_decomp or has_zero_row_sums:\n        laplacian_mtx  = B @ B.T\n        row_sums_mtx   = sp.diags(row_sums)\n        laplacian_mtx  = row_sums_mtx - laplacian_mtx\n        try:\n            #if the row sums are negative, this \n            #step could fail.\n            E_obj = Eigen_Hermitian(laplacian_mtx,\n                                    k=2,\n                                    M=row_sums_mtx,\n                                    sigma=0,\n                                    which=\"LM\")\n            eigen_val_abs = np.abs(E_obj[0])\n            #Identify the eigenvalue with the\n            #largest magnitude.\n            idx = np.argmax(eigen_val_abs)\n            #Choose the eigenvector corresponding\n            # to the eigenvalue with the \n            # largest magnitude.\n            eigen_vectors = E_obj[1]\n            W = eigen_vectors[:,idx]\n        except:\n            #This is a very expensive operation\n            #since it computes all the eigenvectors.\n            if 5000 &lt; n_rows:\n                print(\"We will use a full eigen decomp.\")\n                print(f\"The block size is {n_rows}.\")\n                print(\"Warning ...\")\n                txt = \"This operation is very expensive.\"\n                print(txt)\n            inv_row_sums   = 1/row_sums\n            inv_row_sums   = sp.diags(inv_row_sums)\n            laplacian_mtx  = inv_row_sums @ laplacian_mtx\n            eig_obj = np.linalg.eig(laplacian_mtx)\n            eig_vals = eig_obj.eigenvalues\n            eig_vecs = eig_obj.eigenvectors\n            idx = np.argsort(np.abs(np.real(eig_vals)))\n            idx = idx[1]\n            W = np.real(eig_vecs[:,idx])\n            W = np.squeeze(np.asarray(W))\n\n\n    else:\n        #This is the fast approach.\n        #It is fast in the sense that the \n        #operations are faster if the matrix\n        #is sparse, i.e., O(n) nonzero entries.\n\n        d = 1/np.sqrt(row_sums)\n        D = sp.diags(d)\n        C = D @ B\n        W = self.trunc_SVD.fit_transform(C)\n        singular_values = self.trunc_SVD.singular_values_\n        idx = np.argsort(singular_values)\n        #Get the singular vector corresponding to the\n        #second largest singular value.\n        W = W[:,idx[0]]\n\n\n    mask_c1 = 0 &lt; W\n    mask_c2 = ~mask_c1\n\n    #If one partition has all the elements\n    #then return with Q = 0.\n    if mask_c1.all() or mask_c2.all():\n        return (Q, partition)\n\n    masks = [mask_c1, mask_c2]\n\n    for mask in masks:\n        n_rows_msk = mask.sum()\n        partition.append(rows[mask])\n        ones_msk = ones * mask\n        row_sums_msk = B.T.dot(ones_msk)\n        O_c = row_sums_msk @ row_sums_msk - n_rows_msk\n        L_c = ones_msk @ row_sums  - n_rows_msk\n        Q += O_c / L - (L_c / L)**2\n\n    if self.verbose_mode:\n        print(f'{Q=}')\n        print(f'I found: {partition=}')\n        print('===========================')\n\n    return (Q, partition)\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.convert_graph_to_json","title":"<code>convert_graph_to_json(self)</code>","text":"<p>The graph structure stored in the attribute            self.J has to be formatted into a             JSON file. This function takes care            of that task. The output file is             named 'cluster_tree.json' and is            equivalent to the 'cluster_tree.json'            file produced by too-many-cells.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def convert_graph_to_json(self):\n    \"\"\"\n    The graph structure stored in the attribute\\\n        self.J has to be formatted into a \\\n        JSON file. This function takes care\\\n        of that task. The output file is \\\n        named 'cluster_tree.json' and is\\\n        equivalent to the 'cluster_tree.json'\\\n        file produced by too-many-cells.\n    \"\"\"\n    fname = \"cluster_tree.json\"\n    fname = os.path.join(self.output, fname)\n    s = str(self.J)\n    replace_dict = {' ':'', 'None':'null', \"'\":'\"'}\n    pattern = '|'.join(replace_dict.keys())\n    regexp  = re.compile(pattern)\n    fun = lambda x: replace_dict[x.group(0)] \n    obj = regexp.sub(fun, s)\n    with open(fname, 'w') as output_file:\n        output_file.write(obj)\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.convert_mm_from_source_to_anndata","title":"<code>convert_mm_from_source_to_anndata(self)</code>","text":"<p>This function reads the matrix.mtx file                 located at the source directory.                Since we assume that the matrix                 has the format genes x cells, we                transpose the matrix, then                 convert it to the CSR format                 and then into an AnnData object.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def convert_mm_from_source_to_anndata(self):\n    \"\"\"\n    This function reads the matrix.mtx file \\\n            located at the source directory.\\\n            Since we assume that the matrix \\\n            has the format genes x cells, we\\\n            transpose the matrix, then \\\n            convert it to the CSR format \\\n            and then into an AnnData object.\n    \"\"\"\n\n    self.t0 = clock()\n\n    print('Loading data from .mtx file.')\n    print('Note that we assume the format:')\n    print('genes=rows and cells=columns.')\n\n    fname = None\n    for f in os.listdir(self.source):\n        if f.endswith('.mtx'):\n            fname = f\n            break\n\n    if fname is None:\n        raise ValueError('.mtx file not found.')\n\n    fname = os.path.join(self.source, fname)\n    mat = mmread(fname)\n    #Remember that the input matrix has\n    #genes for rows and cells for columns.\n    #Thus, just transpose.\n    self.A = mat.T.tocsr()\n\n    fname = 'barcodes.tsv'\n    print(f'Loading {fname}')\n    fname = os.path.join(self.source, fname)\n    df_barcodes = pd.read_csv(\n            fname, delimiter='\\t', header=None)\n    barcodes = df_barcodes.loc[:,0].tolist()\n\n    fname = 'genes.tsv'\n    print(f'Loading {fname}')\n    fname = os.path.join(self.source, fname)\n    df_genes = pd.read_csv(\n            fname, delimiter='\\t', header=None)\n    genes = df_genes.loc[:,0].tolist()\n\n    self.A = AnnData(self.A)\n    self.A.obs_names = barcodes\n    self.A.var_names = genes\n\n    self.tf = clock()\n    delta = self.tf - self.t0\n    txt = ('Elapsed time for loading: ' + \n            f'{delta:.2f} seconds.')\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.create_data_for_tmci","title":"<code>create_data_for_tmci(self, tmci_mtx_dir='tmci_mtx_data', list_of_genes=[], path_to_genes='', create_matrix=True)</code>","text":"<p>Produce the 10X files for a given set of            genes.  This function produces the            genes x cells matrix market format matrix,            the genes.tsv file and the barcodes. If a path is provided for the genes, then the            first column of the csv file must have the            gene names.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def create_data_for_tmci(\n        self,\n        tmci_mtx_dir: Optional[str] = \"tmci_mtx_data\",\n        list_of_genes: Optional[list] = [],\n        path_to_genes: Optional[str] = \"\",\n        create_matrix: Optional[bool] = True,\n        ):\n    \"\"\"\n    Produce the 10X files for a given set of\\\n        genes.  This function produces the\\\n        genes x cells matrix market format matrix,\\\n        the genes.tsv file and the barcodes.\n    If a path is provided for the genes, then the\\\n        first column of the csv file must have the\\\n        gene names.\n    \"\"\"\n\n    self.tmci_mtx_dir = os.path.join(\n        self.output, tmci_mtx_dir)\n\n    os.makedirs(self.tmci_mtx_dir, exist_ok=True)\n\n    # Genes\n    genes_f = \"genes.tsv\"\n    genes_f = os.path.join(self.tmci_mtx_dir, genes_f)\n\n    var_names = []\n    col_indices = []\n\n    if 0 &lt; len(path_to_genes):\n        df = pd.read_csv(path_to_genes, header=0)\n        #The first column should contain the genes.\n        list_of_genes = df.iloc[:,0].to_list()\n\n    if 0 &lt; len(list_of_genes):\n\n        for gene in list_of_genes:\n            if gene not in self.A.var.index:\n                continue\n            var_names.append(gene)\n            col_index = self.A.var.index.get_loc(gene)\n            col_indices.append(col_index)\n\n        G_mtx = self.A.X[:,col_indices]\n\n    else:\n        #If not list is provided, use all the genes.\n        var_names = self.A.var_names\n        G_mtx = self.A.X\n\n    L = [var_names,var_names]\n    pd.DataFrame(L).transpose().to_csv(\n        genes_f,\n        sep=\"\\t\",\n        header=False,\n        index=False)\n\n    # Barcodes\n    barcodes_f = \"barcodes.tsv\"\n    barcodes_f = os.path.join(self.tmci_mtx_dir,\n                              barcodes_f)\n    pd.Series(self.A.obs_names).to_csv(\n        barcodes_f,\n        sep=\"\\t\",\n        header=False,\n        index=False)\n\n    # Matrix\n    if create_matrix:\n        matrix_f = \"matrix.mtx\"\n        matrix_f = os.path.join(self.tmci_mtx_dir,\n                                matrix_f)\n        mmwrite(matrix_f, sp.coo_matrix(G_mtx.T))\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.estimate_n_of_iterations","title":"<code>estimate_n_of_iterations(self)</code>","text":"<p>We assume a model of the form         number_of_iter = const * N^exponent         where N is the number of cells.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def estimate_n_of_iterations(self) -&gt; int:\n    \"\"\"\n    We assume a model of the form \\\n    number_of_iter = const * N^exponent \\\n    where N is the number of cells.\n    \"\"\"\n\n    #Average number of cells per leaf node\n    k = np.power(10, -0.6681664297844971)\n    exponent = 0.86121348\n    #exponent = 0.9\n    q1 = k * np.power(self.n_cells, exponent)\n    q2 = 2\n    iter_estimates = np.array([q1,q2], dtype=int)\n\n    return iter_estimates.max()\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.generate_cell_annotation_file","title":"<code>generate_cell_annotation_file(self, cell_ann_col='cell_annotations', tag='cell_annotation_labels')</code>","text":"<p>This function stores a CSV file with            the labels for each cell.</p> <p>:param column: Name of the            column in the .obs data frame of            the AnnData object that contains            the labels to be used for the tree            visualization. For example, cell             types.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def generate_cell_annotation_file(self,\n        cell_ann_col: Optional[str] = \"cell_annotations\",\n        tag: Optional[str]=\"cell_annotation_labels\"\n):\n    \"\"\"\n    This function stores a CSV file with\\\n        the labels for each cell.\n\n    :param column: Name of the\\\n        column in the .obs data frame of\\\n        the AnnData object that contains\\\n        the labels to be used for the tree\\\n        visualization. For example, cell \\\n        types.\n\n    \"\"\"\n    if tag[-3:] == \".csv\":\n        pass\n    else:\n        fname = tag + \".csv\"\n\n    fname = os.path.join(self.output, fname)\n    self.cell_annotations_path = fname\n\n    ca = self.A.obs[cell_ann_col].copy()\n    ca.index.names = ['item']\n    ca = ca.rename('label')\n    ca.to_csv(fname, index=True)\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.generate_matrix_from_signature_file","title":"<code>generate_matrix_from_signature_file(self, signature_path)</code>","text":"<p>Generate a matrix from the signature provided             through a file. The entries with a positive     weight are assumed to be upregulated and             those with a negative weight are assumed             to be downregulated. The algorithm will             standardize the matrix, i.e., centering             and scaling.</p> <p>If the signature has both positive and             negative weights, two versions will be             created. The unadjusted version simply             computes a weighted average using the             weights provided in the signature file.            In the adjusted version the weights             are adjusted to give equal weight to the             upregulated and downregulated genes.</p> <p>Assumptions</p> <p>We assume that the file has at least two             columns. One should be named \"Gene\" and             the other \"Weight\".             The count matrix has cells for rows and             genes for columns.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def generate_matrix_from_signature_file(\n        self,\n        signature_path: str):\n    \"\"\"\n    Generate a matrix from the signature provided \\\n        through a file. The entries with a positive\n        weight are assumed to be upregulated and \\\n        those with a negative weight are assumed \\\n        to be downregulated. The algorithm will \\\n        standardize the matrix, i.e., centering \\\n        and scaling.\n\n    If the signature has both positive and \\\n        negative weights, two versions will be \\\n        created. The unadjusted version simply \\\n        computes a weighted average using the \\\n        weights provided in the signature file.\\\n        In the adjusted version the weights \\\n        are adjusted to give equal weight to the \\\n        upregulated and downregulated genes.\n\n    Assumptions\n\n    We assume that the file has at least two \\\n        columns. One should be named \"Gene\" and \\\n        the other \"Weight\". \\\n        The count matrix has cells for rows and \\\n        genes for columns.\n    \"\"\"\n\n    df_signature = pd.read_csv(signature_path, header=0)\n\n    Z = sc.pp.scale(self.A, copy=True)\n    Z_is_sparse = sp.issparse(Z)\n\n    vec = np.zeros(Z.X.shape[0])\n\n    up_reg = vec * 0\n    down_reg = vec * 0\n\n    up_count = 0\n    up_weight = 0\n\n    down_count = 0\n    down_weight = 0\n\n    G = df_signature[\"Gene\"]\n    W = df_signature[\"Weight\"]\n\n    for gene, weight in zip(G, W):\n        if gene not in Z.var.index:\n            continue\n        col_index = Z.var.index.get_loc(gene)\n\n        if Z_is_sparse:\n            gene_col = Z.X.getcol(col_index)\n            gene_col = np.squeeze(gene_col.toarray())\n        else:\n            gene_col = Z.X[:,col_index]\n\n        if 0 &lt; weight:\n            up_reg += weight * gene_col\n            up_weight += weight\n            up_count += 1\n        else:\n            down_reg += weight * gene_col\n            down_weight += np.abs(weight)\n            down_count += 1\n\n    total_counts = up_count + down_count\n    total_weight = up_weight + down_weight\n\n    list_of_names = []\n    list_of_gvecs = []\n\n    UnAdjSign = up_reg + down_reg\n    UnAdjSign /= total_weight\n    self.A.obs[\"UnAdjSign\"] = UnAdjSign\n    list_of_gvecs.append(UnAdjSign)\n    list_of_names.append(\"UnAdjSign\")\n\n    up_factor = down_count / total_counts\n    down_factor = up_count / total_counts\n\n    modified_total_counts = 2 * up_count * down_count\n    modified_total_counts /= total_counts\n\n    check = up_factor*up_count + down_factor*down_count\n\n    print(f\"{up_count=}\")\n    print(f\"{down_count=}\")\n    print(f\"{total_counts=}\")\n    print(f\"{modified_total_counts=}\")\n    print(f\"{check=}\")\n    print(f\"{up_factor=}\")\n    print(f\"{down_factor=}\")\n\n\n    mixed_signs = True\n    if 0 &lt; up_count:\n        UpReg   = up_reg / up_count\n        self.A.obs[\"UpReg\"] = UpReg\n        list_of_gvecs.append(UpReg)\n        list_of_names.append(\"UpReg\")\n        print(\"UpRegulated genes: stats\")\n        print(self.A.obs[\"UpReg\"].describe())\n\n    else:\n        mixed_signs = False\n\n    if 0 &lt; down_count:\n        DownReg   = down_reg / down_count\n        self.A.obs[\"DownReg\"] = DownReg\n        list_of_gvecs.append(DownReg)\n        list_of_names.append(\"DownReg\")\n        print(\"DownRegulated genes: stats\")\n        print(self.A.obs[\"DownReg\"].describe())\n        txt = (\"Note: In our representation, \" \n               \"the higher the value of a downregulated \"\n               \"gene, the more downregulated it is.\")\n        print(txt)\n    else:\n        mixed_signs = False\n\n    if mixed_signs:\n        AdjSign  = up_factor * up_reg\n        AdjSign += down_factor * down_reg\n        AdjSign /= modified_total_counts\n        self.A.obs[\"AdjSign\"] = AdjSign\n        list_of_gvecs.append(AdjSign)\n        list_of_names.append(\"AdjSign\")\n\n    m = np.vstack(list_of_gvecs)\n\n    #This function will produce the \n    #barcodes.tsv and the genes.tsv file.\n    self.create_data_for_tmci(\n        list_of_genes = list_of_names,\n        create_matrix=False)\n\n\n    m = m.astype(np.float32)\n\n    mtx_path = os.path.join(\n        self.tmci_mtx_dir, \"matrix.mtx\")\n\n    mmwrite(mtx_path, sp.coo_matrix(m))\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.get_path_from_node_x_to_node_y","title":"<code>get_path_from_node_x_to_node_y(self, x, y)</code>","text":"<p>For a given pair of nodes x and y, we find the path between those nodes.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def get_path_from_node_x_to_node_y(\n        self,\n        x: int,\n        y: int,\n        ):\n    \"\"\"\n    For a given pair of nodes x and y, we find the\n    path between those nodes.\n    \"\"\"\n    x_path, x_dist = self.get_path_from_root_to_node(x)\n    y_path, y_dist = self.get_path_from_root_to_node(y)\n\n    x_set = set(x_path)\n    y_set = set(y_path)\n\n    # print(x_dist)\n    # print(y_dist)\n\n    # print(\"===========\")\n\n    # print(x_path)\n    # print(y_path)\n\n    # print(\"===========\")\n\n    intersection = x_set.intersection(y_set)\n    intersection = list(intersection)\n    intersection = np.array(intersection)\n    n_intersection = len(intersection)\n\n    pivot_node = x_path[-n_intersection]\n    pivot_dist = x_dist[-n_intersection]\n\n    x_path = x_path[:-n_intersection]\n    y_path = y_path[:-n_intersection]\n    y_path = y_path[::-1]\n\n    x_dist = x_dist[:-n_intersection]\n    y_dist = y_dist[1:-n_intersection]\n    y_dist = y_dist[::-1]\n\n    full_path = np.hstack((x_path,pivot_node,y_path))\n    full_dist = np.hstack(\n        (x_dist, pivot_dist, pivot_dist, y_dist))\n    full_dist = full_dist.cumsum()\n\n    # print(full_path) \n    # print(full_dist)\n\n    return (full_path, full_dist)\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.get_path_from_root_to_node","title":"<code>get_path_from_root_to_node(self, target)</code>","text":"<p>For a given node, we find the path from the root  to that node.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def get_path_from_root_to_node(\n        self,\n        target: int,\n        ):\n    \"\"\"\n    For a given node, we find the path from the root \n    to that node.\n    \"\"\"\n\n    node = target\n    path_vec = [node]\n    modularity_vec = [0]\n\n    while node != 0:\n        # Get an iterator for the predecessors.\n        # There should only be one predecessor.\n        predecessors = self.G.predecessors(node)\n        node = next(predecessors)\n        Q = self.G._node[node][\"Q\"]\n        Q = float(Q)\n        path_vec.append(node)\n        modularity_vec.append(Q)\n\n    # We assume that the distance between two children\n    # nodes is equal to the modularity of the parent node.\n    # Hence, the distance from a child to a parent is \n    # half the modularity.\n    modularity_vec = 0.5 * np.array(\n        modularity_vec, dtype=float)\n    path_vec = np.array(path_vec, dtype=int)\n\n    return (path_vec, modularity_vec)\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.load_cluster_info","title":"<code>load_cluster_info(self, cluster_file_path='')</code>","text":"<p>Load the cluster file.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def load_cluster_info(\n        self,\n        cluster_file_path: Optional[str]=\"\",\n        ):\n    \"\"\"\n    Load the cluster file.\n    \"\"\"\n\n    self.t0 = clock()\n\n    if 0 &lt; len(cluster_file_path):\n        cluster_fname = cluster_file_path\n\n    else:\n        fname = 'clusters.csv'\n        cluster_fname = os.path.join(self.output, fname)\n\n    if not os.path.exists(cluster_fname):\n        raise ValueError(\"File does not exists.\")\n\n    df = pd.read_csv(cluster_fname, index_col=0)\n    self.A.obs[\"sp_cluster\"] = df[\"cluster\"]\n\n    self.set_of_leaf_nodes = set(df[\"cluster\"])\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.load_graph","title":"<code>load_graph(self, dot_fname='')</code>","text":"<p>Load the dot file.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def load_graph(\n        self,\n        dot_fname: Optional[str]=\"\",\n        ):\n    \"\"\"\n    Load the dot file.\n    \"\"\"\n\n    self.t0 = clock()\n\n\n    if len(dot_fname) == 0:\n        fname = 'graph.dot'\n        dot_fname = os.path.join(self.output, fname)\n\n    if not os.path.exists(dot_fname):\n        raise ValueError(\"File does not exists.\")\n\n    self.G = nx.nx_agraph.read_dot(dot_fname)\n    self.G = nx.DiGraph(self.G)\n    n_nodes = self.G.number_of_nodes()\n\n    # Change string labels to integers.\n    D = {}\n    for k in range(n_nodes):\n        D[str(k)] = k\n\n    self.G = nx.relabel_nodes(self.G, D, copy=True)\n\n    # self.G = nx.convert_node_labels_to_integers(self.G)\n    # Changing the labels to integers using the above \n    # function follows a different numbering scheme to \n    # that given by the labels of the node.\n\n    print(self.G)\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.normalize_dense_rows","title":"<code>normalize_dense_rows(self)</code>","text":"<p>Divide each row of the count matrix by the             given norm. Note that this function             assumes that the matrix is dense.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def normalize_dense_rows(self):\n    \"\"\"\n    Divide each row of the count matrix by the \\\n        given norm. Note that this function \\\n        assumes that the matrix is dense.\n    \"\"\"\n\n    print('Normalizing rows.')\n\n    for row in self.X:\n        row /= np.linalg.norm(row,\n                              ord=self.similarity_norm)\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.normalize_sparse_rows","title":"<code>normalize_sparse_rows(self)</code>","text":"<p>Divide each row of the count matrix by the             given norm. Note that this function             assumes that the matrix is in the             compressed sparse row format.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def normalize_sparse_rows(self):\n    \"\"\"\n    Divide each row of the count matrix by the \\\n        given norm. Note that this function \\\n        assumes that the matrix is in the \\\n        compressed sparse row format.\n    \"\"\"\n\n    print(\"Normalizing rows.\")\n\n\n    #It's just an alias.\n    mat = self.X\n\n    for i in range(self.n_cells):\n        row = mat.getrow(i)\n        nz = row.data\n        row_norm  = np.linalg.norm(\n            nz, ord=self.similarity_norm)\n        row = nz / row_norm\n        mat.data[mat.indptr[i]:mat.indptr[i+1]] = row\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.plot_expression_from_node_x_to_node_y","title":"<code>plot_expression_from_node_x_to_node_y(self, x, y, genes)</code>","text":"<p>For a given pair of nodes x and y, we compute the             gene expression path along the path connecting            those nodes. Make sure that property set_of_leaf_nodes is            populated with the correct information.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def plot_expression_from_node_x_to_node_y(\n        self,\n        x: int,\n        y: int,\n        genes: Union[list, str],\n        ):\n    \"\"\"\n    For a given pair of nodes x and y, we compute the \\\n        gene expression path along the path connecting\\\n        those nodes.\n    Make sure that property set_of_leaf_nodes is\\\n        populated with the correct information.\n    \"\"\"\n\n    if isinstance(genes, str):\n        list_of_genes = [genes]\n    else:\n        list_of_genes = genes\n\n    T = self.get_path_from_node_x_to_node_y(x,y)\n    path_vec, dist_vec = T\n    n_nodes = len(path_vec)\n    n_genes = len(list_of_genes)\n    exp_mat = np.zeros((n_genes,n_nodes))\n\n    for col,node in enumerate(path_vec):\n        g_exp = self.compute_cluster_mean_expression(\n            node, list_of_genes)\n        exp_mat[:,col] = g_exp\n\n    fig,ax = plt.subplots()\n\n    # bogus_names = [\"Gene A\", \"Gene B\"]\n    # colors = [\"blue\", \"red\"]\n\n    for row, gene in enumerate(list_of_genes):\n        ax.plot(dist_vec,\n                exp_mat[row,:],\n                linewidth=3,\n                label=gene,\n                # label=bogus_names[row],\n                # color = colors[row]\n                )\n\n    plt.legend()\n    txt = f\"From node {x} to node {y}\"\n    # txt = f\"From node X to node Y\"\n    ax.set_title(txt)\n    ax.set_ylabel(\"Gene expression\")\n    ax.set_xlabel(\"Distance (modularity units)\")\n    plt.ticklabel_format(style='sci',\n                         axis='x',\n                         scilimits=(0,0))\n\n    fname = \"expression_path.pdf\"\n    fname = os.path.join(self.output, fname)\n    fig.savefig(fname, bbox_inches=\"tight\")\n    print(\"Plot has been generated.\")\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.reverse_path","title":"<code>reverse_path(self, p)</code>","text":"<p>This function reverses the path from the root        node to the leaf node.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def reverse_path(self, p: str)-&gt;str:\n    \"\"\"\n    This function reverses the path from the root\\\n    node to the leaf node.\n    \"\"\"\n    reversed_p = \"/\".join(p.split(\"/\")[::-1])\n    return reversed_p\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.run_spectral_clustering","title":"<code>run_spectral_clustering(self, shift_similarity_matrix=0, normalize_rows=False, similarity_function='cosine_sparse', similarity_norm=2, similarity_power=1, similarity_gamma=None, use_eig_decomp=False, use_tf_idf=False, tf_idf_norm=None, tf_idf_smooth=True, svd_algorithm='randomized')</code>","text":"<p>This function computes the partitions of the                 initial cell population and continues                 until the modularity of the newly                 created partitions is nonpositive.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def run_spectral_clustering(\n        self,\n        shift_similarity_matrix:Optional[float] = 0,\n        normalize_rows:Optional[bool] = False,\n        similarity_function:Optional[str]=\"cosine_sparse\",\n        similarity_norm: Optional[float] = 2,\n        similarity_power: Optional[float] = 1,\n        similarity_gamma: Optional[float] = None,\n        use_eig_decomp: Optional[bool] = False,\n        use_tf_idf: Optional[bool] = False,\n        tf_idf_norm: Optional[str] = None,\n        tf_idf_smooth: Optional[str] = True,\n        svd_algorithm: Optional[str] = \"randomized\"):\n    \"\"\"\n    This function computes the partitions of the \\\n            initial cell population and continues \\\n            until the modularity of the newly \\\n            created partitions is nonpositive.\n    \"\"\"\n\n    svd_algorithms = [\"randomized\",\"arpack\"]\n    if svd_algorithm not in svd_algorithms:\n        raise ValueError(\"Unexpected SVD algorithm.\")\n\n    if similarity_norm &lt; 1:\n        raise ValueError(\"Unexpected similarity norm.\")\n    self.similarity_norm = similarity_norm\n\n    if similarity_gamma is None:\n        # gamma = 1 / (number of features)\n        similarity_gamma = 1 / self.X.shape[1]\n    elif similarity_gamma &lt;= 0:\n        raise ValueError(\"Unexpected similarity gamma.\")\n\n    if similarity_power &lt;= 0:\n        raise ValueError(\"Unexpected similarity power.\")\n\n    similarity_functions = []\n    similarity_functions.append(\"cosine_sparse\")\n    similarity_functions.append(\"cosine\")\n    similarity_functions.append(\"neg_exp\")\n    similarity_functions.append(\"laplacian\")\n    similarity_functions.append(\"gaussian\")\n    similarity_functions.append(\"div_by_sum\")\n    if similarity_function not in similarity_functions:\n        raise ValueError(\"Unexpected similarity fun.\")\n\n\n    #In case the user wants to call this function again.\n    self.spectral_clustering_has_been_called = True\n\n    #TF-IDF section\n    if use_tf_idf:\n\n        t0 = clock()\n        print(\"Using inverse document frequency (IDF).\")\n\n        if tf_idf_norm is None:\n            pass \n        else:\n            print(\"Using term frequency normalization.\")\n            tf_idf_norms = [\"l2\",\"l1\"]\n            if tf_idf_norm not in tf_idf_norms:\n                raise ValueError(\"Unexpected tf norm.\")\n\n        tf_idf_obj = TfidfTransformer(\n            norm=tf_idf_norm,\n            smooth_idf=tf_idf_smooth)\n\n        self.X = tf_idf_obj.fit_transform(self.X)\n        if self.is_sparse:\n            pass\n        else:\n            #If the matrix was originally dense\n            #and the tf_idf function changed it\n            #to sparse, then convert to dense.\n            if sp.issparse(self.X):\n                self.X = self.X.toarray()\n\n        tf = clock()\n        delta = tf - t0\n        txt = (\"Elapsed time for IDF build: \" +\n                f\"{delta:.2f} seconds.\")\n        print(txt)\n\n    #Normalization section\n    use_cos_sp = similarity_function == \"cosine_sparse\"\n    use_dbs = similarity_function == \"div_by_sum\"\n    if normalize_rows or use_cos_sp or use_dbs:\n        t0 = clock()\n\n        if self.is_sparse:\n            self.normalize_sparse_rows()\n        else:\n            self.normalize_dense_rows()\n\n        tf = clock()\n        delta = tf - t0\n        txt = (\"Elapsed time for normalization: \" +\n                f\"{delta:.2f} seconds.\")\n        print(txt)\n\n    #Similarity section.\n    print(f\"Working with {similarity_function=}\")\n\n    if similarity_function == \"cosine_sparse\":\n\n        self.trunc_SVD = TruncatedSVD(\n                n_components=2,\n                n_iter=5,\n                algorithm=svd_algorithm)\n\n    else:\n        #Use a similarity function different from\n        #the cosine_sparse similarity function.\n\n        t0 = clock()\n        print(\"Building similarity matrix ...\")\n        n_rows = self.X.shape[0]\n        max_workers = os.cpu_count()\n        n_workers = 1\n        if n_rows &lt; 500:\n            pass\n        elif n_rows &lt; 5000:\n            if 8 &lt; max_workers:\n                n_workers = 8\n        elif n_rows &lt; 50000:\n            if 16 &lt; max_workers:\n                n_workers = 16\n        else:\n            if 25 &lt; max_workers:\n                n_workers = 25\n        print(f\"Using {n_workers=}.\")\n\n    if similarity_function == \"cosine_sparse\":\n        pass\n    elif similarity_function == \"cosine\":\n        #( x @ y ) / ( ||x|| * ||y|| )\n        def sim_fun(x,y):\n            cos_sim = x @ y\n            x_norm = np.linalg.norm(x, ord=2)\n            y_norm = np.linalg.norm(y, ord=2)\n            cos_sim /= (x_norm * y_norm)\n            return cos_sim\n\n        self.X = pairwise_kernels(self.X,\n                                    metric=\"cosine\",\n                                    n_jobs=n_workers)\n\n    elif similarity_function == \"neg_exp\":\n        #exp(-||x-y||^power * gamma)\n        def sim_fun(x,y):\n            delta = np.linalg.norm(\n                x-y, ord=similarity_norm)\n            delta = np.power(delta, similarity_power)\n            return np.exp(-delta * similarity_gamma)\n\n        self.X = pairwise_kernels(\n            self.X,\n            metric=sim_fun,\n            n_jobs=n_workers)\n\n    elif similarity_function == \"laplacian\":\n        #exp(-||x-y||^power * gamma)\n        def sim_fun(x,y):\n            delta = np.linalg.norm(\n                x-y, ord=1)\n            delta = np.power(delta, 1)\n            return np.exp(-delta * similarity_gamma)\n\n        self.X = pairwise_kernels(\n            self.X,\n            metric=\"laplacian\",\n            n_jobs=n_workers,\n            gamma = similarity_gamma)\n\n    elif similarity_function == \"gaussian\":\n        #exp(-||x-y||^power * gamma)\n        def sim_fun(x,y):\n            delta = np.linalg.norm(\n                x-y, ord=2)\n            delta = np.power(delta, 2)\n            return np.exp(-delta * similarity_gamma)\n\n        self.X = pairwise_kernels(\n            self.X,\n            metric=\"rbf\",\n            n_jobs=n_workers,\n            gamma = similarity_gamma)\n\n    elif similarity_function == \"div_by_sum\":\n        #1 - ( ||x-y|| / (||x|| + ||y||) )^power\n        #The rows should have been previously normalized.\n        def sim_fun(x,y):\n            delta = np.linalg.norm(\n                x-y, ord=similarity_norm)\n            x_norm = np.linalg.norm(\n                x, ord=similarity_norm)\n            y_norm = np.linalg.norm(\n                y, ord=similarity_norm)\n            delta /= (x_norm + y_norm)\n            delta = np.power(delta, 1)\n            value =  1 - delta\n            return value\n\n        if self.similarity_norm == 1:\n            lp_norm = \"l1\"\n        elif self.similarity_norm == 2:\n            lp_norm = \"l2\"\n        else:\n            txt = \"Similarity norm should be 1 or 2.\"\n            raise ValueError(txt)\n\n        self.X = pairwise_distances(self.X,\n                                    metric=lp_norm,\n                                    n_jobs=n_workers)\n        self.X *= -0.5\n        self.X += 1\n\n    if similarity_function != \"cosine_sparse\":\n\n        if shift_similarity_matrix != 0:\n            print(f\"Similarity matrix will be shifted.\")\n            print(f\"Shift: {shift_similarity_matrix}.\")\n            self.X += shift_similarity_matrix\n\n        print(\"Similarity matrix has been built.\")\n        tf = clock()\n        delta = tf - t0\n        delta /= 60\n        txt = (\"Elapsed time for similarity build: \" +\n                f\"{delta:.2f} minutes.\")\n        print(txt)\n\n\n    self.use_eig_decomp = use_eig_decomp\n\n    self.t0 = clock()\n\n    #===========================================\n    #=============Main=Loop=====================\n    #===========================================\n    node_id = self.node_counter\n\n    #Initialize the array of cells to partition\n    rows = np.array(range(self.X.shape[0]))\n\n    #Initialize the deque\n    # self.DQ.append((rows, None))\n    # self.DQ.append(rows)\n\n    #Initialize the graph\n    self.G.add_node(node_id, size=len(rows))\n\n    #Path to reach root node.\n    self.node_to_path[node_id] = str(node_id)\n\n    #Indices to reach root node.\n    self.node_to_j_index[node_id] = (1,)\n\n    #Update the node counter\n    self.node_counter += 1\n\n    #============STEP=1================Cluster(0)\n\n    p_node_id = node_id\n\n    if similarity_function == \"cosine_sparse\":\n        Q,S = self.compute_partition_for_sp(rows)\n    else:\n        Q,S = self.compute_partition_for_gen(rows)\n\n    if self.eps &lt; Q:\n        #Modularity is above threshold, and\n        #thus each partition will be \n        #inserted into the deque.\n\n        D = self.modularity_to_json(Q)\n\n        #Update json index\n        self.J.append(D)\n        self.J.append([])\n        # self.J.append([[],[]])\n        # j_index = (1,)\n\n        self.G.nodes[node_id]['Q'] = Q\n\n        for indices in S:\n            T = (indices, p_node_id)\n            self.DQ.append(T)\n\n    else:\n        #Modularity is below threshold and \n        #therefore this partition will not\n        #be considered.\n        txt = (\"All cells belong\" \n                \" to the same partition.\")\n        print(txt)\n        return\n\n    max_n_iter = self.estimate_n_of_iterations()\n\n    self.print_message_before_clustering()\n\n    with tqdm(total=max_n_iter) as pbar:\n        while 0 &lt; len(self.DQ):\n\n            #Get the rows corresponding to the\n            #partition and the (parent) node\n            #that produced such partition.\n            rows, p_node_id = self.DQ.pop()\n\n            #This id is for the new node.\n            node_id += 1\n\n            # For every cluster of cells that is popped\n            # from the deque, we update the node_id. \n            # If the cluster is further partitioned we \n            # will store each partition but will not \n            # assign node numbers. Node numbers will \n            # only be assigned after being popped from \n            # the deque.\n\n            # We need to know the modularity to \n            # determine if the node will \n            if similarity_function == \"cosine_sparse\":\n                Q,S = self.compute_partition_for_sp(rows)\n            else:\n                Q,S = self.compute_partition_for_gen(rows)\n\n            # If the parent node is 0, then the path is\n            # \"0\".\n            current_path = self.node_to_path[p_node_id]\n\n            #Update path for the new node\n            new_path = current_path \n            new_path += '/' + str(node_id) \n            self.node_to_path[node_id]=new_path\n\n            # If the parent node is 0, then j_index is\n            # (1,)\n            j_index = self.node_to_j_index[p_node_id]\n\n            n_stored_blocks = len(self.J[j_index])\n            self.J[j_index].append([])\n            #Update the j_index. For example, if\n            #j_index = (1,) and no blocks have been\n            #stored, then the new j_index is (1,0).\n            #Otherwise, it is (1,1).\n            j_index += (n_stored_blocks,)\n\n            #Include new node into the graph.\n            self.G.add_node(node_id, size=len(rows))\n\n            #Include new edge into the graph.\n            self.G.add_edge(p_node_id, node_id)\n\n            if self.eps &lt; Q:\n                #Modularity is above threshold, and\n                #thus each partition will be \n                #inserted into the deque.\n\n                D = self.modularity_to_json(Q)\n                self.J[j_index].append(D)\n                self.J[j_index].append([])\n                j_index += (1,)\n\n                # We only store the modularity of nodes\n                # whose modularity is above threshold.\n                self.G.nodes[node_id]['Q'] = Q\n\n                # Update the j_index for the newly \n                # created node. (1,0,1)\n                self.node_to_j_index[node_id] = j_index\n\n                # Append each partition to the deque.\n                for indices in S:\n                    T = (indices, node_id)\n                    self.DQ.append(T)\n\n            else:\n                #Modularity is below threshold and \n                #therefore this partition will not\n                #be considered.\n\n                #Update the relation between a set of\n                #cells and the corresponding leaf node.\n                #Also include the path to reach that node.\n                c = self.cluster_column_index\n                self.A.obs.iloc[rows, c] = node_id\n\n                reversed_path = self.reverse_path(\n                    new_path)\n                p = self.path_column_index\n                self.A.obs.iloc[rows, p] = reversed_path\n\n                self.set_of_leaf_nodes.add(node_id)\n\n                #Update the JSON structure for \n                #a leaf node.\n                L = self.cells_to_json(rows)\n                self.J[j_index].append(L)\n                self.J[j_index].append([])\n\n            pbar.update()\n\n        #==============END OF WHILE==============\n        pbar.total = pbar.n\n        self.final_n_iter = pbar.n\n        pbar.refresh()\n\n    self.tf = clock()\n    self.delta_clustering = self.tf - self.t0\n    self.delta_clustering /= 60\n    txt = (\"Elapsed time for clustering: \" +\n            f\"{self.delta_clustering:.2f} minutes.\")\n    print(txt)\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.store_outputs","title":"<code>store_outputs(self, cell_ann_col='cell_annotations')</code>","text":"<p>Store the outputs and plot the branching tree.</p> <p>File outputs:</p> <p>cluster_list.json: The json file containing a list  of clusters. </p> <p>cluster_tree.json: The json file containing the  output tree in a recursive format.</p> <p>graph.dot: A dot file of the tree. It includes the  modularity and the size.</p> <p>node_info.csv: Size and modularity of each node.</p> <p>clusters.csv: The cluster membership for each cell.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def store_outputs(\n        self,\n        cell_ann_col: Optional[str] = \"cell_annotations\",\n        ):\n    \"\"\"\n    Store the outputs and plot the branching tree.\n\n    File outputs:\n\n    cluster_list.json: The json file containing a list \n    of clusters. \n\n    cluster_tree.json: The json file containing the \n    output tree in a recursive format.\n\n    graph.dot: A dot file of the tree. It includes the \n    modularity and the size.\n\n    node_info.csv: Size and modularity of each node.\n\n    clusters.csv: The cluster membership for each cell.\n\n    \"\"\"\n\n    self.t0 = clock()\n\n\n    fname = 'graph.dot'\n    dot_fname = os.path.join(self.output, fname)\n\n    nx.nx_agraph.write_dot(self.G, dot_fname)\n    #Write cell to node data frame.\n    self.write_cell_assignment_to_csv()\n    self.convert_graph_to_json()\n    self.write_cluster_list_to_json()\n\n    #Store the cell annotations in the output folder.\n    if 0 &lt; len(cell_ann_col):\n        if cell_ann_col in self.A.obs.columns:\n            self.generate_cell_annotation_file(\n                cell_ann_col)\n        else:\n            txt = \"Annotation column does not exists.\"\n            #raise ValueError(txt)\n\n    print(self.G)\n\n    #Number of cells for each node\n    size_list = []\n    #Modularity for each node\n    Q_list = []\n    #Node label\n    node_list = []\n\n    for node, attr in self.G.nodes(data=True):\n        node_list.append(node)\n        size_list.append(attr['size'])\n        if 'Q' in attr:\n            Q_list.append(attr['Q'])\n        else:\n            Q_list.append(np.nan)\n\n    #Write node information to CSV\n    D = {'node': node_list, 'size':size_list, 'Q':Q_list}\n    df = pd.DataFrame(D)\n    fname = 'node_info.csv'\n    fname = os.path.join(self.output, fname)\n    df.to_csv(fname, index=False)\n\n    if self.use_twopi_cmd:\n        self.plot_radial_tree_from_dot_file()\n\n    self.tf = clock()\n    delta = self.tf - self.t0\n    txt = ('Elapsed time for storing outputs: ' +\n            f'{delta:.2f} seconds.')\n    print(txt)\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.update_cell_annotations","title":"<code>update_cell_annotations(self, df, column='cell_annotations')</code>","text":"<p>Insert a column of cell annotations in the         AnnData.obs data frame. The column in the         data frame should be called \"label\". The         name of the column in the AnnData.obs         data frame is provided by the user through         the column argument.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def update_cell_annotations(\n        self,\n        df: pd.DataFrame,\n        column: str = \"cell_annotations\"):\n    \"\"\"\n    Insert a column of cell annotations in the \\\n    AnnData.obs data frame. The column in the \\\n    data frame should be called \"label\". The \\\n    name of the column in the AnnData.obs \\\n    data frame is provided by the user through \\\n    the column argument.\n    \"\"\"\n\n    if \"label\" not in df.columns:\n        raise ValueError(\"Missing label column.\")\n\n    #Reindex the data frame.\n    df = df.loc[self.A.obs.index]\n\n    if df.shape[0] != self.A.obs.shape[0]:\n        raise ValueError(\"Data frame size mismatch.\")\n\n    self.A.obs[column] =  df[\"label\"]\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.visualize_with_tmc_interactive","title":"<code>visualize_with_tmc_interactive(self, path_to_tmc_interactive, use_column_for_labels='', port=9991, include_matrix_data=False, tmci_mtx_dir='')</code>","text":"<p>This function produces a visualization                using too-many-cells-interactive.</p> <p>:param path_to_tmc_interactive: Path to                 the too-many-cells-interactive                 directory. :param use_column_for_labels: Name of the                column in the .obs data frame of                the AnnData object that contains                the labels to be used in the tree                visualization. For example, cell                 types. :param port: Port to be used to open                the app in your browser using                the address localhost:port.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def visualize_with_tmc_interactive(self,\n        path_to_tmc_interactive: str,\n        use_column_for_labels: Optional[str] = \"\",\n        port: Optional[int] = 9991,\n        include_matrix_data: Optional[bool] = False,\n        tmci_mtx_dir: Optional[str] = \"\",\n        ) -&gt; None:\n    \"\"\"\n    This function produces a visualization\\\n            using too-many-cells-interactive.\n\n    :param path_to_tmc_interactive: Path to \\\n            the too-many-cells-interactive \\\n            directory.\n    :param use_column_for_labels: Name of the\\\n            column in the .obs data frame of\\\n            the AnnData object that contains\\\n            the labels to be used in the tree\\\n            visualization. For example, cell \\\n            types.\n    :param port: Port to be used to open\\\n            the app in your browser using\\\n            the address localhost:port.\n\n    \"\"\"\n\n    fname = \"cluster_tree.json\"\n    fname = os.path.join(self.output, fname)\n    tree_path = fname\n    port_str = str(port)\n\n\n    bash_exec = \"./start-and-load.sh\"\n\n\n    if len(use_column_for_labels) == 0:\n        label_path_str = \"\"\n        label_path     = \"\"\n    else:\n        self.generate_cell_annotation_file(\n                use_column_for_labels)\n        label_path_str = \"--label-path\"\n        label_path     = self.cell_annotations_path\n\n    if include_matrix_data:\n        matrix_path_str = \"--matrix-dir\"\n        if 0 &lt; len(tmci_mtx_dir):\n            matrix_dir = tmci_mtx_dir\n        else:\n\n            if len(self.tmci_mtx_dir) == 0:\n                print(\"No path for TMCI mtx.\")\n                print(\"Creating TMCI mtx data.\")\n                self.create_data_for_tmci()\n\n            matrix_dir = self.tmci_mtx_dir\n    else:\n        matrix_path_str = \"\"\n        matrix_dir = \"\"\n\n    command = [\n            bash_exec,\n            matrix_path_str,\n            matrix_dir,\n            '--tree-path',\n            tree_path,\n            label_path_str,\n            label_path,\n            '--port',\n            port_str\n            ]\n\n    command = list(filter(len,command))\n    command = ' '.join(command)\n\n    #Run the command as if we were inside the\n    #too-many-cells-interactive folder.\n    final_command = (f\"(cd {path_to_tmc_interactive} \"\n            f\"&amp;&amp; {command})\")\n    #print(final_command)\n    url = 'localhost:' + port_str\n    txt = (\"Once the app is running, just type in \"\n            f\"your browser \\n        {url}\")\n    print(txt)\n    txt=\"The app will start loading after pressing Enter.\"\n    print(txt)\n    pause = input('Press Enter to continue ...')\n    p = subprocess.call(final_command, shell=True)\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.write_cell_assignment_to_csv","title":"<code>write_cell_assignment_to_csv(self)</code>","text":"<p>This function creates a CSV file that indicates             the assignment of each cell to a specific             cluster. The first column is the cell id,             the second column is the cluster id, and             the third column is the path from the root             node to the given node.</p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def write_cell_assignment_to_csv(self):\n    \"\"\"\n    This function creates a CSV file that indicates \\\n        the assignment of each cell to a specific \\\n        cluster. The first column is the cell id, \\\n        the second column is the cluster id, and \\\n        the third column is the path from the root \\\n        node to the given node.\n    \"\"\"\n    fname = 'clusters.csv'\n    fname = os.path.join(self.output, fname)\n    labels = ['sp_cluster','sp_path']\n    df = self.A.obs[labels]\n    df.index.names = ['cell']\n    df = df.rename(columns={'sp_cluster':'cluster',\n                            'sp_path':'path'})\n    df.to_csv(fname, index=True)\n</code></pre>"},{"location":"toomanycells/#toomanycells.toomanycells.TooManyCells.write_cluster_list_to_json","title":"<code>write_cluster_list_to_json(self)</code>","text":"<p>This function creates a JSON file that indicates             the assignment of each cell to a specific             cluster. </p> Source code in <code>toomanycells/toomanycells.py</code> <pre><code>def write_cluster_list_to_json(self):\n    \"\"\"\n    This function creates a JSON file that indicates \\\n        the assignment of each cell to a specific \\\n        cluster. \n    \"\"\"\n    fname = 'cluster_list.json'\n    fname = os.path.join(self.output, fname)\n    master_list = []\n    relevant_cols = [\"sp_cluster\", \"sp_path\"]\n    df = self.A.obs[relevant_cols]\n    df = df.reset_index(names=\"cell\")\n    df = df.sort_values([\"sp_cluster\",\"cell\"])\n    for idx, row in df.iterrows():\n        cluster = row[\"sp_cluster\"]\n        path_str= row[\"sp_path\"]\n        cell    = row[\"cell\"]\n        nodes = path_str.split(\"/\")\n        list_of_nodes = []\n        sub_dict_1 = {\"unCell\":cell}\n        sub_dict_2 = {\"unRow\":idx}\n        main_dict = {\"_barcode\":sub_dict_1,\n                     \"_cellRow\":sub_dict_2}\n        for node in nodes:\n            d = {\"unCluster\":int(node)}\n            list_of_nodes.append(d)\n\n        master_list.append([main_dict, list_of_nodes])\n\n    s = str(master_list)\n    replace_dict = {' ':'', \"'\":'\"'}\n    pattern = '|'.join(replace_dict.keys())\n    regexp  = re.compile(pattern)\n    fun = lambda x: replace_dict[x.group(0)] \n    obj = regexp.sub(fun, s)\n    with open(fname, 'w') as output_file:\n        output_file.write(obj)\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use toomanycells in a project:</p> <pre><code>from toomanycells import TooManyCells as tmc\n</code></pre>"},{"location":"examples/intro/","title":"Intro","text":"In\u00a0[1]: Copied! <pre>print('Hello World!')\n</pre> print('Hello World!') <pre>Hello World!\n</pre>"}]}